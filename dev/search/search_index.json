{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling Features \u00b6 Time Series Prediction \u00b6 TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more. Effective HorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more. Analytics \u00b6 Analytics model analyzes the workload and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas. QoS Ensurance \u00b6 Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more. Load-aware Scheduling \u00b6 Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems. In contrast, Crane-scheduler can get the actual load of kubernetes nodes from Prometheus, and achieve more efficient scheduling. Please see this document to learn more. Repositories \u00b6 Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - A Kubernetes scheduler which can schedule pod based on actual node load.","title":"Introduction"},{"location":"#introduction","text":"The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling","title":"Introduction"},{"location":"#features","text":"","title":"Features"},{"location":"#time-series-prediction","text":"TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more.","title":"Time Series Prediction"},{"location":"#effective-horizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more.","title":"Effective HorizontalPodAutoscaler"},{"location":"#analytics","text":"Analytics model analyzes the workload and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas.","title":"Analytics"},{"location":"#qos-ensurance","text":"Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more.","title":"QoS Ensurance"},{"location":"#load-aware-scheduling","text":"Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems. In contrast, Crane-scheduler can get the actual load of kubernetes nodes from Prometheus, and achieve more efficient scheduling. Please see this document to learn more.","title":"Load-aware Scheduling"},{"location":"#repositories","text":"Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - A Kubernetes scheduler which can schedule pod based on actual node load.","title":"Repositories"},{"location":"CONTRIBUTING/","text":"Contributing to Crane \u00b6 Welcome to Crane! This document is a guideline about how to contribute to Crane. Become a contributor \u00b6 You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides . Report bugs \u00b6 Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug. Suggest enhancements \u00b6 If you have an idea to improve Crane, submit an feature request .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-crane","text":"Welcome to Crane! This document is a guideline about how to contribute to Crane.","title":"Contributing to Crane"},{"location":"CONTRIBUTING/#become-a-contributor","text":"You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides .","title":"Become a contributor"},{"location":"CONTRIBUTING/#report-bugs","text":"Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug.","title":"Report bugs"},{"location":"CONTRIBUTING/#suggest-enhancements","text":"If you have an idea to improve Crane, submit an feature request .","title":"Suggest enhancements"},{"location":"code-standards/","text":"Code standards \u00b6 This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project import need to be organized \u00b6 import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" ) logs standard \u00b6 logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err )) event is needed for critical reconcile loop \u00b6 event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ()) comment \u00b6 every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) } functions \u00b6 function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils variable \u00b6 variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop folder and file \u00b6 folder name should be letter with lower case and number file name should be letter and number and _ unit test \u00b6 Test-driven developing Complex function that include condition decide should add unit test for it don't forget to run make fmt before you submit code \u00b6","title":"Code Standard"},{"location":"code-standards/#code-standards","text":"This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project","title":"Code standards"},{"location":"code-standards/#import-need-to-be-organized","text":"import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" )","title":"import need to be organized"},{"location":"code-standards/#logs-standard","text":"logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err ))","title":"logs standard"},{"location":"code-standards/#event-is-needed-for-critical-reconcile-loop","text":"event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ())","title":"event is needed for critical reconcile loop"},{"location":"code-standards/#comment","text":"every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) }","title":"comment"},{"location":"code-standards/#functions","text":"function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils","title":"functions"},{"location":"code-standards/#variable","text":"variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop","title":"variable"},{"location":"code-standards/#folder-and-file","text":"folder name should be letter with lower case and number file name should be letter and number and _","title":"folder and file"},{"location":"code-standards/#unit-test","text":"Test-driven developing Complex function that include condition decide should add unit test for it","title":"unit test"},{"location":"code-standards/#dont-forget-to-run-make-fmt-before-you-submit-code","text":"","title":"don't forget to run make fmt before you submit code"},{"location":"installation/","text":"Installation \u00b6 Prerequisites \u00b6 Kubernetes 1.18+ Helm 3.1.0 Steps \u00b6 Helm Installation \u00b6 Please refer to Helm's documentation for installation. Installing prometheus and grafana with helm chart \u00b6 Note If you already deployed prometheus, grafana in your environment, then skip this step. Network Problems If your network is hard to connect GitHub resources, you can try the mirror repo. Like GitHub Release, GitHub Raw Content raw.githubusercontent.com . But mirror repo has a certain latency . Mirror Repo Crane use prometheus to be the default metric provider. Using following command to install prometheus components: prometheus-server, node-exporter, kube-state-metrics. Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Fadvisor use grafana to present cost estimates. Using following command to install a grafana. Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana Deploying Crane and Fadvisor \u00b6 Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor Deploying Crane-scheduler(optional) \u00b6 helm install scheduler -n crane-system --create-namespace crane/scheduler Verify Installation \u00b6 Check deployments are all available by running: kubectl get deploy -n crane-system The output is similar to: NAME READY STATUS RESTARTS AGE crane-agent-8h7df 1 /1 Running 0 119m crane-agent-8qf5n 1 /1 Running 0 119m crane-agent-h9h5d 1 /1 Running 0 119m craned-5c69c684d8-dxmhw 2 /2 Running 0 20m grafana-7fddd867b4-kdxv2 1 /1 Running 0 41m metric-adapter-94b6f75b-k8h7z 1 /1 Running 0 119m prometheus-kube-state-metrics-6dbc9cd6c9-dfmkw 1 /1 Running 0 45m prometheus-node-exporter-bfv74 1 /1 Running 0 45m prometheus-node-exporter-s6zps 1 /1 Running 0 45m prometheus-node-exporter-x5rnm 1 /1 Running 0 45m prometheus-server-5966b646fd-g9vxl 2 /2 Running 0 45m you can see this to learn more. Customize Installation \u00b6 Deploy Crane by apply YAML declaration. Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter The following command will configure prometheus http address for crane if you want to customize it. Specify CUSTOMIZE_PROMETHEUS if you have existing prometheus server. export CUSTOMIZE_PROMETHEUS = if [ $CUSTOMIZE_PROMETHEUS ] ; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/ ${ CUSTOMIZE_PROMETHEUS } /\" deploy/craned/deployment.yaml ; fi Get your Kubernetes Cost Report \u00b6 Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace crane-system -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace crane-system port-forward $POD_NAME 3000 visit Cost Report here with account(admin:admin).","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#prerequisites","text":"Kubernetes 1.18+ Helm 3.1.0","title":"Prerequisites"},{"location":"installation/#steps","text":"","title":"Steps"},{"location":"installation/#helm-installation","text":"Please refer to Helm's documentation for installation.","title":"Helm Installation"},{"location":"installation/#installing-prometheus-and-grafana-with-helm-chart","text":"Note If you already deployed prometheus, grafana in your environment, then skip this step. Network Problems If your network is hard to connect GitHub resources, you can try the mirror repo. Like GitHub Release, GitHub Raw Content raw.githubusercontent.com . But mirror repo has a certain latency . Mirror Repo Crane use prometheus to be the default metric provider. Using following command to install prometheus components: prometheus-server, node-exporter, kube-state-metrics. Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Fadvisor use grafana to present cost estimates. Using following command to install a grafana. Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana","title":"Installing prometheus and grafana with helm chart"},{"location":"installation/#deploying-crane-and-fadvisor","text":"Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor","title":"Deploying Crane and Fadvisor"},{"location":"installation/#deploying-crane-scheduleroptional","text":"helm install scheduler -n crane-system --create-namespace crane/scheduler","title":"Deploying Crane-scheduler(optional)"},{"location":"installation/#verify-installation","text":"Check deployments are all available by running: kubectl get deploy -n crane-system The output is similar to: NAME READY STATUS RESTARTS AGE crane-agent-8h7df 1 /1 Running 0 119m crane-agent-8qf5n 1 /1 Running 0 119m crane-agent-h9h5d 1 /1 Running 0 119m craned-5c69c684d8-dxmhw 2 /2 Running 0 20m grafana-7fddd867b4-kdxv2 1 /1 Running 0 41m metric-adapter-94b6f75b-k8h7z 1 /1 Running 0 119m prometheus-kube-state-metrics-6dbc9cd6c9-dfmkw 1 /1 Running 0 45m prometheus-node-exporter-bfv74 1 /1 Running 0 45m prometheus-node-exporter-s6zps 1 /1 Running 0 45m prometheus-node-exporter-x5rnm 1 /1 Running 0 45m prometheus-server-5966b646fd-g9vxl 2 /2 Running 0 45m you can see this to learn more.","title":"Verify Installation"},{"location":"installation/#customize-installation","text":"Deploy Crane by apply YAML declaration. Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter The following command will configure prometheus http address for crane if you want to customize it. Specify CUSTOMIZE_PROMETHEUS if you have existing prometheus server. export CUSTOMIZE_PROMETHEUS = if [ $CUSTOMIZE_PROMETHEUS ] ; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/ ${ CUSTOMIZE_PROMETHEUS } /\" deploy/craned/deployment.yaml ; fi","title":"Customize Installation"},{"location":"installation/#get-your-kubernetes-cost-report","text":"Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace crane-system -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace crane-system port-forward $POD_NAME 3000 visit Cost Report here with account(admin:admin).","title":"Get your Kubernetes Cost Report"},{"location":"mirror/","text":"Mirror Repo \u00b6 About mirror repo \u00b6 Because of various network issues, it is difficult to access GitHub resources such as GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com in some regions. For a better experience, GoCrane offers several additional mirror repositories for you, but with some latency. Helm Resources \u00b6 Tips Sync the latest version of upstream every six hours Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public Git Resources \u00b6 Tips Sync upstream repository every day Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public Get the raw file contents of the Coding repo \u00b6 Here you'll find out how to get the contents of a source file directly from the Coding Git repository via an HTTP request. Coding Git Repo - Key Params \u00b6 Similar to regular API requests, the Coding Git repository provides a corresponding API interface. The following is an overview of the related parameters. Example Using https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml as an example. Click Here Params Description example team Name of the team finops project Name of the project gocrane repo Name of the Git Repo helm-charts branch Name of the branch main file path The path to the file in the repo /integration/grafana/override_values.yaml Constructing HTTP requests \u00b6 By filling in the following URL construction rules according to the properties mentioned above, you can obtain a URL that can directly access the content of the source file. https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips Try this command. curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"Mirror Repo"},{"location":"mirror/#mirror-repo","text":"","title":"Mirror Repo"},{"location":"mirror/#about-mirror-repo","text":"Because of various network issues, it is difficult to access GitHub resources such as GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com in some regions. For a better experience, GoCrane offers several additional mirror repositories for you, but with some latency.","title":"About mirror repo"},{"location":"mirror/#helm-resources","text":"Tips Sync the latest version of upstream every six hours Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public","title":"Helm Resources"},{"location":"mirror/#git-resources","text":"Tips Sync upstream repository every day Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public","title":"Git Resources"},{"location":"mirror/#get-the-raw-file-contents-of-the-coding-repo","text":"Here you'll find out how to get the contents of a source file directly from the Coding Git repository via an HTTP request.","title":"Get the raw file contents of the Coding repo"},{"location":"mirror/#coding-git-repo-key-params","text":"Similar to regular API requests, the Coding Git repository provides a corresponding API interface. The following is an overview of the related parameters. Example Using https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml as an example. Click Here Params Description example team Name of the team finops project Name of the project gocrane repo Name of the Git Repo helm-charts branch Name of the branch main file path The path to the file in the repo /integration/grafana/override_values.yaml","title":"Coding Git Repo - Key Params"},{"location":"mirror/#constructing-http-requests","text":"By filling in the following URL construction rules according to the properties mentioned above, you can obtain a URL that can directly access the content of the source file. https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips Try this command. curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"Constructing HTTP requests"},{"location":"proposals/20220228-advanced-cpuset-manger/","text":"Advanced CPUSet Manager \u00b6 Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet. Table of Contents \u00b6 Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations Motivation \u00b6 Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service. Goals \u00b6 Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus Non-Goals/Future Work \u00b6 Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered Proposal \u00b6 Relax restrictions of cpuset allocation \u00b6 Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2 Add new annotation to describe the requirement of cpuset contorl manger \u00b6 apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use. Advanced CPU Manager component \u00b6 Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet User Stories \u00b6 Users can update pod annotaion to control cpuset policy flexibly Story 1 \u00b6 make pod from none to share without recreating pod Story 2 \u00b6 make pod from exclusive to share, so offline process can use these CPUs Risks and Mitigations \u00b6 kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Advanced CpuSet Manager"},{"location":"proposals/20220228-advanced-cpuset-manger/#advanced-cpuset-manager","text":"Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet.","title":"Advanced CPUSet Manager"},{"location":"proposals/20220228-advanced-cpuset-manger/#table-of-contents","text":"Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations","title":"Table of Contents"},{"location":"proposals/20220228-advanced-cpuset-manger/#motivation","text":"Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service.","title":"Motivation"},{"location":"proposals/20220228-advanced-cpuset-manger/#goals","text":"Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus","title":"Goals"},{"location":"proposals/20220228-advanced-cpuset-manger/#non-goalsfuture-work","text":"Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered","title":"Non-Goals/Future Work"},{"location":"proposals/20220228-advanced-cpuset-manger/#proposal","text":"","title":"Proposal"},{"location":"proposals/20220228-advanced-cpuset-manger/#relax-restrictions-of-cpuset-allocation","text":"Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2","title":"Relax restrictions of cpuset allocation"},{"location":"proposals/20220228-advanced-cpuset-manger/#add-new-annotation-to-describe-the-requirement-of-cpuset-contorl-manger","text":"apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use.","title":"Add new annotation to describe the  requirement of cpuset contorl manger"},{"location":"proposals/20220228-advanced-cpuset-manger/#advanced-cpu-manager-component","text":"Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet","title":"Advanced CPU Manager component"},{"location":"proposals/20220228-advanced-cpuset-manger/#user-stories","text":"Users can update pod annotaion to control cpuset policy flexibly","title":"User Stories"},{"location":"proposals/20220228-advanced-cpuset-manger/#story-1","text":"make pod from none to share without recreating pod","title":"Story 1"},{"location":"proposals/20220228-advanced-cpuset-manger/#story-2","text":"make pod from exclusive to share, so offline process can use these CPUs","title":"Story 2"},{"location":"proposals/20220228-advanced-cpuset-manger/#risks-and-mitigations","text":"kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Risks and Mitigations"},{"location":"roadmaps/roadmap-1h-2022/","text":"Crane Roadmap for H1 2022 \u00b6 Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan. 0.1.0 [released] \u00b6 Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing 0.2.0\uff1a[released] \u00b6 Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio 0.3.0 [released] \u00b6 UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA 0.4.0 [April] \u00b6 EVPA support Dynamic Scheduler UI to support EPA. 0.5.0 [May] \u00b6 HPC open source Node & Pod QoS Ensurance for DiskIO and Network Prediction with DiskIO, Network 0.6.0 [June] \u00b6 Scalability to support 3k TSP and 3k EPA Application Portrait SLO based Application QoS for CPU and Mem","title":"1H 2022"},{"location":"roadmaps/roadmap-1h-2022/#crane-roadmap-for-h1-2022","text":"Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan.","title":"Crane Roadmap for H1 2022"},{"location":"roadmaps/roadmap-1h-2022/#010-released","text":"Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing","title":"0.1.0 [released]"},{"location":"roadmaps/roadmap-1h-2022/#020released","text":"Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio","title":"0.2.0\uff1a[released]"},{"location":"roadmaps/roadmap-1h-2022/#030-released","text":"UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA","title":"0.3.0 [released]"},{"location":"roadmaps/roadmap-1h-2022/#040-april","text":"EVPA support Dynamic Scheduler UI to support EPA.","title":"0.4.0 [April]"},{"location":"roadmaps/roadmap-1h-2022/#050-may","text":"HPC open source Node & Pod QoS Ensurance for DiskIO and Network Prediction with DiskIO, Network","title":"0.5.0 [May]"},{"location":"roadmaps/roadmap-1h-2022/#060-june","text":"Scalability to support 3k TSP and 3k EPA Application Portrait SLO based Application QoS for CPU and Mem","title":"0.6.0 [June]"},{"location":"tutorials/analytics-and-recommendation/","text":"Analytics and Recommendation \u00b6 Analytics and Recommendation provide capacity that analyzes the workload in k8s cluster and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas. Analytics and Recommend Pod Resources \u00b6 Create an Resource Analytics to give recommendation for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics -n crane-system analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-resource 15m metric-adapter-resource 15m You can get created recommendation from analytics status: kubectl get analytics craned-resource -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : Resource status : lastSuccessfulTime : \"2022-01-12T08:40:59Z\" recommendations : - name : craned-resource-resource-j7shb namespace : crane-system uid : 8ce2eedc-7969-4b80-8aee-fd4a98d6a8b6 The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-j7shb -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-resource-resource-j7shb namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-resource uid : a9e6dc0d-ab26-4f2a-84bd-4fe9e0f3e105 spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : Resource status : conditions : - lastTransitionTime : \"2022-01-12T08:40:59Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastSuccessfulTime : \"2022-01-12T08:40:59Z\" lastUpdateTime : \"2022-01-12T08:40:59Z\" resourceRequest : containers : - containerName : craned target : cpu : 114m memory : 120586239m The status.resourceRequest is recommended by crane's recommendation engine. Something you should know about Resource recommendation: Resource Recommendation use historic prometheus metrics to calculate and propose. We use Percentile algorithm to process metrics that also used by VPA. If the workload is running for a long term like several weeks, the result will be more accurate. Analytics and Recommend HPA \u00b6 Create an HPA Analytics to give recommendations for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-hpa.yaml kubectl get analytics -n crane-system analytics-hpa.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-hpa 5m52s craned-resource 18h metric-adapter-hpa 5m52s metric-adapter-resource 18h You can get created recommendation from analytics status: kubectl get analytics craned-hpa -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : HPA status : lastSuccessfulTime : \"2022-01-13T07:26:18Z\" recommendations : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation name : craned-hpa-hpa-2f22w namespace : crane-system uid : 397733ee-986a-4630-af75-736d2b58bfac The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-2f22w -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-hpa-hpa-2f22w namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-hpa uid : b216d9c3-c52e-4c9c-b9e9-9d5b45165b1d spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : HPA status : conditions : - lastTransitionTime : \"2022-01-13T07:51:18Z\" message : 'Failed to offer recommend, Recommendation crane-system/craned-hpa-hpa-2f22w error EHPAAdvisor prediction metrics data is unexpected, List length is 0 ' reason : FailedOfferRecommend status : \"False\" type : Ready lastUpdateTime : \"2022-01-13T07:51:18Z\" The status.resourceRequest is recommended by crane's recommendation engine. The fail reason is demo workload don't have enough run time. Something you should know about HPA recommendation: HPA Recommendation use historic prometheus metrics to calculate, forecast and propose. We use DSP algorithm to process metrics. We recommend using Effective HorizontalPodAutoscaler to execute autoscaling, you can see this document to learn more. The Workload need match following conditions: Existing at least one ready pod Ready pod ratio should larger that 50% Must provide cpu request for pod spec The workload should be running for at least a week to get enough metrics to forecast The workload's cpu load should be predictable, too low or too unstable workload often is unpredictable","title":"Analytics and Recommendation"},{"location":"tutorials/analytics-and-recommendation/#analytics-and-recommendation","text":"Analytics and Recommendation provide capacity that analyzes the workload in k8s cluster and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas.","title":"Analytics and Recommendation"},{"location":"tutorials/analytics-and-recommendation/#analytics-and-recommend-pod-resources","text":"Create an Resource Analytics to give recommendation for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics -n crane-system analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-resource 15m metric-adapter-resource 15m You can get created recommendation from analytics status: kubectl get analytics craned-resource -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : Resource status : lastSuccessfulTime : \"2022-01-12T08:40:59Z\" recommendations : - name : craned-resource-resource-j7shb namespace : crane-system uid : 8ce2eedc-7969-4b80-8aee-fd4a98d6a8b6 The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-j7shb -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-resource-resource-j7shb namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-resource uid : a9e6dc0d-ab26-4f2a-84bd-4fe9e0f3e105 spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : Resource status : conditions : - lastTransitionTime : \"2022-01-12T08:40:59Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastSuccessfulTime : \"2022-01-12T08:40:59Z\" lastUpdateTime : \"2022-01-12T08:40:59Z\" resourceRequest : containers : - containerName : craned target : cpu : 114m memory : 120586239m The status.resourceRequest is recommended by crane's recommendation engine. Something you should know about Resource recommendation: Resource Recommendation use historic prometheus metrics to calculate and propose. We use Percentile algorithm to process metrics that also used by VPA. If the workload is running for a long term like several weeks, the result will be more accurate.","title":"Analytics and Recommend Pod Resources"},{"location":"tutorials/analytics-and-recommendation/#analytics-and-recommend-hpa","text":"Create an HPA Analytics to give recommendations for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-hpa.yaml kubectl get analytics -n crane-system analytics-hpa.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-hpa 5m52s craned-resource 18h metric-adapter-hpa 5m52s metric-adapter-resource 18h You can get created recommendation from analytics status: kubectl get analytics craned-hpa -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : HPA status : lastSuccessfulTime : \"2022-01-13T07:26:18Z\" recommendations : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation name : craned-hpa-hpa-2f22w namespace : crane-system uid : 397733ee-986a-4630-af75-736d2b58bfac The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-2f22w -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-hpa-hpa-2f22w namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-hpa uid : b216d9c3-c52e-4c9c-b9e9-9d5b45165b1d spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : HPA status : conditions : - lastTransitionTime : \"2022-01-13T07:51:18Z\" message : 'Failed to offer recommend, Recommendation crane-system/craned-hpa-hpa-2f22w error EHPAAdvisor prediction metrics data is unexpected, List length is 0 ' reason : FailedOfferRecommend status : \"False\" type : Ready lastUpdateTime : \"2022-01-13T07:51:18Z\" The status.resourceRequest is recommended by crane's recommendation engine. The fail reason is demo workload don't have enough run time. Something you should know about HPA recommendation: HPA Recommendation use historic prometheus metrics to calculate, forecast and propose. We use DSP algorithm to process metrics. We recommend using Effective HorizontalPodAutoscaler to execute autoscaling, you can see this document to learn more. The Workload need match following conditions: Existing at least one ready pod Ready pod ratio should larger that 50% Must provide cpu request for pod spec The workload should be running for at least a week to get enough metrics to forecast The workload's cpu load should be predictable, too low or too unstable workload often is unpredictable","title":"Analytics and Recommend HPA"},{"location":"tutorials/dynamic-scheduler-plugin/","text":"Dynamic-scheduler: a load-aware scheduler plugin \u00b6 Introduction \u00b6 Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems: for some nodes, the actual load is not much different from the resource request, which will lead to a very high probability of stability problems. for others, the actual load is much smaller than the resource request, which will lead to a huge waste of resources. To solve these problems, Dynamic scheduler builds a simple but efficient model based on actual node utilization data\uff0cand filters out those nodes with high load to balance the cluster. Design Details \u00b6 Architecture \u00b6 As shown above, Dynamic scheduler relies on Prometheus and Node-exporter to collect and aggregate metrics data, and it consists of two components: Note Node-annotator is currently a module of Crane-scheduler-controller . Node-annotator periodically pulls data from Prometheus and marks them with timestamp on the node in the form of annotations. Dynamic plugin reads the load data directly from the node's annotation, filters and scores candidates based on a simple algorithm. Scheduler Policy \u00b6 Dynamic provides a default scheduler policy and supports user-defined policies. The default policy reies on following metrics: cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d At the scheduling Filter stage, the node will be filtered if the actual usage rate of this node is greater than the threshold of any of the above metrics. And at the Score stage, the final score is the weighted sum of these metrics' values. Hot Value \u00b6 In the production cluster, scheduling hotspots may occur frequently because the load of the nodes can not increase immediately after the pod is created. Therefore, we define an extra metrics named Hot Value , which represents the scheduling frequency of the node in recent times. And the final priority of the node is the final score minus the Hot Value .","title":"Dynamic-scheduler: a load-aware scheduler plugin"},{"location":"tutorials/dynamic-scheduler-plugin/#dynamic-scheduler-a-load-aware-scheduler-plugin","text":"","title":"Dynamic-scheduler: a load-aware scheduler plugin"},{"location":"tutorials/dynamic-scheduler-plugin/#introduction","text":"Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems: for some nodes, the actual load is not much different from the resource request, which will lead to a very high probability of stability problems. for others, the actual load is much smaller than the resource request, which will lead to a huge waste of resources. To solve these problems, Dynamic scheduler builds a simple but efficient model based on actual node utilization data\uff0cand filters out those nodes with high load to balance the cluster.","title":"Introduction"},{"location":"tutorials/dynamic-scheduler-plugin/#design-details","text":"","title":"Design Details"},{"location":"tutorials/dynamic-scheduler-plugin/#architecture","text":"As shown above, Dynamic scheduler relies on Prometheus and Node-exporter to collect and aggregate metrics data, and it consists of two components: Note Node-annotator is currently a module of Crane-scheduler-controller . Node-annotator periodically pulls data from Prometheus and marks them with timestamp on the node in the form of annotations. Dynamic plugin reads the load data directly from the node's annotation, filters and scores candidates based on a simple algorithm.","title":"Architecture"},{"location":"tutorials/dynamic-scheduler-plugin/#scheduler-policy","text":"Dynamic provides a default scheduler policy and supports user-defined policies. The default policy reies on following metrics: cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d At the scheduling Filter stage, the node will be filtered if the actual usage rate of this node is greater than the threshold of any of the above metrics. And at the Score stage, the final score is the weighted sum of these metrics' values.","title":"Scheduler Policy"},{"location":"tutorials/dynamic-scheduler-plugin/#hot-value","text":"In the production cluster, scheduling hotspots may occur frequently because the load of the nodes can not increase immediately after the pod is created. Therefore, we define an extra metrics named Hot Value , which represents the scheduling frequency of the node in recent times. And the final priority of the node is the final score minus the Hot Value .","title":"Hot Value"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/","text":"Crane-scheduler \u00b6 Overview \u00b6 Crane-scheduler is a collection of scheduler plugins based on scheduler framework , including: Dynamic scheuler: a load-aware scheduler plugin Get Started \u00b6 1. Install Prometheus \u00b6 Make sure your kubernetes cluster has Prometheus installed. If not, please refer to Install Prometheus . 2. Configure Prometheus Rules \u00b6 1) Configure the rules of Prometheus to get expected aggregated data: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \u26a0\ufe0fTroubleshooting: The sampling interval of Prometheus must be less than 30 seconds, otherwise the above rules(such as cpu_usage_active) may not take effect. 2) Update the configuration of Prometheus service discovery to ensure that node_exporters/telegraf are using node name as instance name: - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note: This step can be skipped if the node name itself is the host IP. 3. Install Crane-scheduler \u00b6 There are two options: 1) Install Crane-scheduler as a second scheduler: helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler 2) Replace native Kube-scheduler with Crane-scheduler: 1) Backup /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ 2) Modify configfile of kube-scheduler( scheduler-config.yaml ) to enable Dynamic scheduler plugin and configure plugin args: apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... 3) Create /etc/kubernetes/policy.yaml , using as scheduler policy of Dynamic plugin: apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 4) Modify kube-scheduler.yaml and replace kube-scheduler image with Crane-scheduler\uff1a ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... 1) Install crane-scheduler-controller : kubectl apply ./deploy/controller/rbac.yaml && kubectl apply -f ./deploy/controller/deployment.yaml 4. Schedule Pods With Crane-scheduler \u00b6 Test Crane-scheduler with following example: apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note: Change crane-scheduler to default-scheduler if crane-scheduler is used as default. There will be the following event if the test pod is successfully scheduled: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"Load-aware Scheduling"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler","text":"","title":"Crane-scheduler"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#overview","text":"Crane-scheduler is a collection of scheduler plugins based on scheduler framework , including: Dynamic scheuler: a load-aware scheduler plugin","title":"Overview"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#get-started","text":"","title":"Get Started"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#1-install-prometheus","text":"Make sure your kubernetes cluster has Prometheus installed. If not, please refer to Install Prometheus .","title":"1. Install Prometheus"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#2-configure-prometheus-rules","text":"1) Configure the rules of Prometheus to get expected aggregated data: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \u26a0\ufe0fTroubleshooting: The sampling interval of Prometheus must be less than 30 seconds, otherwise the above rules(such as cpu_usage_active) may not take effect. 2) Update the configuration of Prometheus service discovery to ensure that node_exporters/telegraf are using node name as instance name: - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note: This step can be skipped if the node name itself is the host IP.","title":"2. Configure Prometheus Rules"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#3-install-crane-scheduler","text":"There are two options: 1) Install Crane-scheduler as a second scheduler: helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler 2) Replace native Kube-scheduler with Crane-scheduler: 1) Backup /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ 2) Modify configfile of kube-scheduler( scheduler-config.yaml ) to enable Dynamic scheduler plugin and configure plugin args: apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... 3) Create /etc/kubernetes/policy.yaml , using as scheduler policy of Dynamic plugin: apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 4) Modify kube-scheduler.yaml and replace kube-scheduler image with Crane-scheduler\uff1a ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... 1) Install crane-scheduler-controller : kubectl apply ./deploy/controller/rbac.yaml && kubectl apply -f ./deploy/controller/deployment.yaml","title":"3. Install Crane-scheduler"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#4-schedule-pods-with-crane-scheduler","text":"Test Crane-scheduler with following example: apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note: Change crane-scheduler to default-scheduler if crane-scheduler is used as default. There will be the following event if the test pod is successfully scheduled: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"4. Schedule Pods With Crane-scheduler"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/","text":"EffectiveHorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with HorizontalPodAutoscaler but extends more features. EffectiveHorizontalPodAutoscaler supports prediction-driven autoscaling. With this capability, user can forecast the incoming peak flow and scale up their application ahead, also user can know when the peak flow will end and scale down their application gracefully. Besides that, EffectiveHorizontalPodAutoscaler also defines several scale strategies to support different scaling scenarios. Features \u00b6 A EffectiveHorizontalPodAutoscaler sample yaml looks like below: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef is the reference to the workload that should be scaled. MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. ScaleStrategy indicates the strategy to scaling target, value can be \"Auto\" and \"Preview\". Metrics contains the specifications for which to use to calculate the desired replica count. Prediction defines configurations for predict resources.If unspecified, defaults don't enable prediction. PredictionWindowSeconds is the time window to predict metrics in the future. Prediction-driven autoscaling \u00b6 Most of online applications follow regular pattern. We can predict future trend of hours or days. DSP is a time series prediction algorithm that applicable for application metrics prediction. The following shows a sample EffectiveHorizontalPodAutoscaler yaml with prediction enabled. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" Metric conversion \u00b6 When user defines spec.metrics in EffectiveHorizontalPodAutoscaler and prediction configuration is enabled, EffectiveHPAController will convert it to a new metric and configure the background HorizontalPodAutoscaler. This is a source EffectiveHorizontalPodAutoscaler yaml for metric definition. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 It's converted to underlying HorizontalPodAutoscaler metrics yaml. apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource In this sample, the resource metric defined by user is converted into two metrics: prediction metric and origin metric. prediction metric is custom metrics that provided by component MetricAdapter. Since custom metric doesn't support targetAverageUtilization , it's converted to targetAverageValue based on target pod cpu request. origin metric is equivalent to user defined metrics in EffectiveHorizontalPodAutoscaler, to fall back to baseline user defined in case of some unexpected situation e.g. business traffic sudden growth. HorizontalPodAutoscaler will calculate on each metric, and propose new replicas based on that. The largest one will be picked as the new scale. Horizontal scaling process \u00b6 There are six steps of prediction and scaling process: EffectiveHPAController create HorizontalPodAutoscaler and TimeSeriesPrediction instance PredictionCore get historic metric from prometheus and persist into TimeSeriesPrediction HPAController read metrics from KubeApiServer KubeApiServer forward requests to MetricAdapter and MetricServer HPAController calculate all metric results and propose a new scale replicas for target HPAController scale target with Scale Api Below is the process flow. Use case \u00b6 Let's take one use case that using EffectiveHorizontalPodAutoscaler in production cluster. We did a profiling on the load history of one application in production and replayed it in staging environment. With the same application, we leverage both EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler to manage the scale and compare the result. From the red line in below chart, we can see its actual total cpu usage is high at ~8am, ~12pm, ~8pm and low in midnight. The green line shows the prediction cpu usage trend. Below is the comparison result between EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler. The red line is the replica number generated by HorizontalPodAutoscaler and the green line is the result from EffectiveHorizontalPodAutoscaler. We can see significant improvement with EffectiveHorizontalPodAutoscaler: Scale up in advance before peek flow Scale down gracefully after peek flow Fewer replicas changes than HorizontalPodAutoscaler ScaleStrategy \u00b6 EffectiveHorizontalPodAutoscaler provides two strategies for scaling: Auto and Preview . User can change the strategy at runtime, and it will take effect on the fly. Auto \u00b6 Auto strategy achieves automatic scaling based on metrics. It is the default strategy. With this strategy, EffectiveHorizontalPodAutoscaler will create and control a HorizontalPodAutoscaler instance in backend. We don't recommend explicit configuration on the underlying HorizontalPodAutoscaler because it will be overridden by EffectiveHPAController. If user delete EffectiveHorizontalPodAutoscaler, HorizontalPodAutoscaler will be cleaned up too. Preview \u00b6 Preview strategy means EffectiveHorizontalPodAutoscaler won't change target's replicas automatically, so you can preview the calculated replicas and control target's replicas by themselves. User can switch from default strategy to this one by applying spec.scaleStrategy to Preview . It will take effect immediately, During the switch, EffectiveHPAController will disable HorizontalPodAutoscaler if exists and scale the target to the value spec.specificReplicas , if user not set spec.specificReplicas , when ScaleStrategy is change to Preview, it will just stop scaling. A sample preview configuration looks like following: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target HorizontalPodAutoscaler compatible \u00b6 EffectiveHorizontalPodAutoscaler is designed to be compatible with k8s native HorizontalPodAutoscaler, because we don't reinvent the autoscaling part but take advantage of the extension from HorizontalPodAutoscaler and build a high level autoscaling CRD. EffectiveHorizontalPodAutoscaler support all abilities from HorizontalPodAutoscaler like metricSpec and behavior. EffectiveHorizontalPodAutoscaler will continue support incoming new feature from HorizontalPodAutoscaler. EffectiveHorizontalPodAutoscaler status \u00b6 This is a yaml from EffectiveHorizontalPodAutoscaler.Status apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0","title":"Effective HPA"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with HorizontalPodAutoscaler but extends more features. EffectiveHorizontalPodAutoscaler supports prediction-driven autoscaling. With this capability, user can forecast the incoming peak flow and scale up their application ahead, also user can know when the peak flow will end and scale down their application gracefully. Besides that, EffectiveHorizontalPodAutoscaler also defines several scale strategies to support different scaling scenarios.","title":"EffectiveHorizontalPodAutoscaler"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#features","text":"A EffectiveHorizontalPodAutoscaler sample yaml looks like below: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef is the reference to the workload that should be scaled. MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. ScaleStrategy indicates the strategy to scaling target, value can be \"Auto\" and \"Preview\". Metrics contains the specifications for which to use to calculate the desired replica count. Prediction defines configurations for predict resources.If unspecified, defaults don't enable prediction. PredictionWindowSeconds is the time window to predict metrics in the future.","title":"Features"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#prediction-driven-autoscaling","text":"Most of online applications follow regular pattern. We can predict future trend of hours or days. DSP is a time series prediction algorithm that applicable for application metrics prediction. The following shows a sample EffectiveHorizontalPodAutoscaler yaml with prediction enabled. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\"","title":"Prediction-driven autoscaling"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#metric-conversion","text":"When user defines spec.metrics in EffectiveHorizontalPodAutoscaler and prediction configuration is enabled, EffectiveHPAController will convert it to a new metric and configure the background HorizontalPodAutoscaler. This is a source EffectiveHorizontalPodAutoscaler yaml for metric definition. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 It's converted to underlying HorizontalPodAutoscaler metrics yaml. apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource In this sample, the resource metric defined by user is converted into two metrics: prediction metric and origin metric. prediction metric is custom metrics that provided by component MetricAdapter. Since custom metric doesn't support targetAverageUtilization , it's converted to targetAverageValue based on target pod cpu request. origin metric is equivalent to user defined metrics in EffectiveHorizontalPodAutoscaler, to fall back to baseline user defined in case of some unexpected situation e.g. business traffic sudden growth. HorizontalPodAutoscaler will calculate on each metric, and propose new replicas based on that. The largest one will be picked as the new scale.","title":"Metric conversion"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontal-scaling-process","text":"There are six steps of prediction and scaling process: EffectiveHPAController create HorizontalPodAutoscaler and TimeSeriesPrediction instance PredictionCore get historic metric from prometheus and persist into TimeSeriesPrediction HPAController read metrics from KubeApiServer KubeApiServer forward requests to MetricAdapter and MetricServer HPAController calculate all metric results and propose a new scale replicas for target HPAController scale target with Scale Api Below is the process flow.","title":"Horizontal scaling process"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#use-case","text":"Let's take one use case that using EffectiveHorizontalPodAutoscaler in production cluster. We did a profiling on the load history of one application in production and replayed it in staging environment. With the same application, we leverage both EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler to manage the scale and compare the result. From the red line in below chart, we can see its actual total cpu usage is high at ~8am, ~12pm, ~8pm and low in midnight. The green line shows the prediction cpu usage trend. Below is the comparison result between EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler. The red line is the replica number generated by HorizontalPodAutoscaler and the green line is the result from EffectiveHorizontalPodAutoscaler. We can see significant improvement with EffectiveHorizontalPodAutoscaler: Scale up in advance before peek flow Scale down gracefully after peek flow Fewer replicas changes than HorizontalPodAutoscaler","title":"Use case"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#scalestrategy","text":"EffectiveHorizontalPodAutoscaler provides two strategies for scaling: Auto and Preview . User can change the strategy at runtime, and it will take effect on the fly.","title":"ScaleStrategy"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#auto","text":"Auto strategy achieves automatic scaling based on metrics. It is the default strategy. With this strategy, EffectiveHorizontalPodAutoscaler will create and control a HorizontalPodAutoscaler instance in backend. We don't recommend explicit configuration on the underlying HorizontalPodAutoscaler because it will be overridden by EffectiveHPAController. If user delete EffectiveHorizontalPodAutoscaler, HorizontalPodAutoscaler will be cleaned up too.","title":"Auto"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#preview","text":"Preview strategy means EffectiveHorizontalPodAutoscaler won't change target's replicas automatically, so you can preview the calculated replicas and control target's replicas by themselves. User can switch from default strategy to this one by applying spec.scaleStrategy to Preview . It will take effect immediately, During the switch, EffectiveHPAController will disable HorizontalPodAutoscaler if exists and scale the target to the value spec.specificReplicas , if user not set spec.specificReplicas , when ScaleStrategy is change to Preview, it will just stop scaling. A sample preview configuration looks like following: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target","title":"Preview"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontalpodautoscaler-compatible","text":"EffectiveHorizontalPodAutoscaler is designed to be compatible with k8s native HorizontalPodAutoscaler, because we don't reinvent the autoscaling part but take advantage of the extension from HorizontalPodAutoscaler and build a high level autoscaling CRD. EffectiveHorizontalPodAutoscaler support all abilities from HorizontalPodAutoscaler like metricSpec and behavior. EffectiveHorizontalPodAutoscaler will continue support incoming new feature from HorizontalPodAutoscaler.","title":"HorizontalPodAutoscaler compatible"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler-status","text":"This is a yaml from EffectiveHorizontalPodAutoscaler.Status apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0","title":"EffectiveHorizontalPodAutoscaler status"},{"location":"tutorials/using-qos-ensurance/","text":"Qos Ensurance \u00b6 QoS ensurance guarantees the stability of the pods running on Kubernetes. Disable schedule, throttle, evict will be applied to low priority pods when the higher priority pods is impacted by resource competition. Qos Ensurance Architecture \u00b6 Qos ensurance's architecture is shown as below. It contains three modules. state collector: collect metrics periodically anomaly analyzer: analyze the node triggered anomaly used collected metrics action executor: execute avoidance actions, include disable scheduling, throttle and eviction. The main process: State collector synchronizes policies from kube-apiserver. If the policies are changed, the state collector updates the collectors. State collector collects metrics periodically. State collector transmits metrics to anomaly analyzer. Anomaly analyzer ranges all rules to analyze the avoidance threshold or the restored threshold reached. Anomaly analyzer merges the analyzed results and notices the avoidance actions. Action executor executes actions based on the analyzed results. Disable Scheduling \u00b6 The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, disable schedule action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 # The minimum wait time of the node from scheduling disable status to normal status apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) We consider the rule is triggered, when the threshold reached continued so many times We consider the rule is restored, when the threshold not reached continued so many times Name of AvoidanceAction which be associated Strategy for the action, you can set it \"Preview\" to not perform actually Name of metric Threshold of metric Please check the video to learn more about the scheduling disable actions. Throttle \u00b6 The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, throttle action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" The minimal ratio of the CPU quota, if the pod is throttled lower than this ratio, it will be set to this. The step for throttle action. It will reduce this percentage of CPU quota in each avoidance triggered.It will increase this percentage of CPU quota in each restored. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000 Eviction \u00b6 The following YAML is another case, low priority pods on the node will be evicted, when the node CPU usage trigger the threshold. apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" Duration in seconds the pod needs to terminate gracefully. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 Strategy for the action, \"Preview\" to not perform actually Supported Metrics \u00b6 Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Qos Ensurance"},{"location":"tutorials/using-qos-ensurance/#qos-ensurance","text":"QoS ensurance guarantees the stability of the pods running on Kubernetes. Disable schedule, throttle, evict will be applied to low priority pods when the higher priority pods is impacted by resource competition.","title":"Qos Ensurance"},{"location":"tutorials/using-qos-ensurance/#qos-ensurance-architecture","text":"Qos ensurance's architecture is shown as below. It contains three modules. state collector: collect metrics periodically anomaly analyzer: analyze the node triggered anomaly used collected metrics action executor: execute avoidance actions, include disable scheduling, throttle and eviction. The main process: State collector synchronizes policies from kube-apiserver. If the policies are changed, the state collector updates the collectors. State collector collects metrics periodically. State collector transmits metrics to anomaly analyzer. Anomaly analyzer ranges all rules to analyze the avoidance threshold or the restored threshold reached. Anomaly analyzer merges the analyzed results and notices the avoidance actions. Action executor executes actions based on the analyzed results.","title":"Qos Ensurance Architecture"},{"location":"tutorials/using-qos-ensurance/#disable-scheduling","text":"The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, disable schedule action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 # The minimum wait time of the node from scheduling disable status to normal status apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) We consider the rule is triggered, when the threshold reached continued so many times We consider the rule is restored, when the threshold not reached continued so many times Name of AvoidanceAction which be associated Strategy for the action, you can set it \"Preview\" to not perform actually Name of metric Threshold of metric Please check the video to learn more about the scheduling disable actions.","title":"Disable Scheduling"},{"location":"tutorials/using-qos-ensurance/#throttle","text":"The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, throttle action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" The minimal ratio of the CPU quota, if the pod is throttled lower than this ratio, it will be set to this. The step for throttle action. It will reduce this percentage of CPU quota in each avoidance triggered.It will increase this percentage of CPU quota in each restored. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000","title":"Throttle"},{"location":"tutorials/using-qos-ensurance/#eviction","text":"The following YAML is another case, low priority pods on the node will be evicted, when the node CPU usage trigger the threshold. apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" Duration in seconds the pod needs to terminate gracefully. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 Strategy for the action, \"Preview\" to not perform actually","title":"Eviction"},{"location":"tutorials/using-qos-ensurance/#supported-metrics","text":"Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Supported Metrics"},{"location":"tutorials/using-time-series-prediction/","text":"TimeSeriesPrediction \u00b6 Knowing the future makes things easier for us. Many businesses are naturally cyclical in time series, especially for those that directly or indirectly serve \"people\". This periodicity is determined by the regularity of people\u2019s daily activities. For example, people are accustomed to ordering take-out at noon and in the evenings; there are always traffic peaks in the morning and evening; even for services that don't have such obvious patterns, such as searching, the amount of requests at night is much lower than that during business hours. For applications related to this kind of business, it is a natural idea to infer the next day's metrics from the historical data of the past few days, or to infer the coming Monday's access traffic from the data of last Monday. With predicted metrics or traffic patterns in the next 24 hours, we can better manage our application instances, stabilize our system, and meanwhile, reduce the cost. TimeSeriesPrediction is used to forecast the kubernetes object metric. It is based on PredictionCore to do forecast. Features \u00b6 A TimeSeriesPrediction sample yaml looks like below: apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef defines the reference to the kubernetes object including Node or other workload such as Deployment. spec.predictionMetrics defines the metrics about the spec.targetRef. spec.predictionWindowSeconds is a prediction time series duration. the TimeSeriesPredictionController will rotate the predicted data in spec.Status for consumer to consume the predicted time series data. PredictionMetrics \u00b6 apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" MetricType \u00b6 There are three types of the metric query: ResourceQuery is a kubernetes built-in resource metric such as cpu or memory. crane supports only cpu and memory now. RawQuery is a query by DSL, such as prometheus query language. now support prometheus. ExpressionQuery is a query by Expression selector. Now we only support prometheus as data source. We define the MetricType to orthogonal with the datasource. but now maybe some datasources do not support the metricType. Algorithm \u00b6 Algorithm define the algorithm type and params to do predict for the metric. Now there are two kinds of algorithms: dsp is an algorithm to forcasting a time series, it is based on FFT(Fast Fourier Transform), it is good at predicting some time series with seasonality and periods. percentile is an algorithm to estimate a time series, and find a recommended value to represent the past time series, it is based on exponentially-decaying weights historgram statistics. it is used to estimate a time series, it is not good at to predict a time sequences, although the percentile can output a time series predicted data, but it is all the same value. so if you want to predict a time sequences, dsp is a better choice. dsp params \u00b6 percentile params \u00b6","title":"Time Series Prediction"},{"location":"tutorials/using-time-series-prediction/#timeseriesprediction","text":"Knowing the future makes things easier for us. Many businesses are naturally cyclical in time series, especially for those that directly or indirectly serve \"people\". This periodicity is determined by the regularity of people\u2019s daily activities. For example, people are accustomed to ordering take-out at noon and in the evenings; there are always traffic peaks in the morning and evening; even for services that don't have such obvious patterns, such as searching, the amount of requests at night is much lower than that during business hours. For applications related to this kind of business, it is a natural idea to infer the next day's metrics from the historical data of the past few days, or to infer the coming Monday's access traffic from the data of last Monday. With predicted metrics or traffic patterns in the next 24 hours, we can better manage our application instances, stabilize our system, and meanwhile, reduce the cost. TimeSeriesPrediction is used to forecast the kubernetes object metric. It is based on PredictionCore to do forecast.","title":"TimeSeriesPrediction"},{"location":"tutorials/using-time-series-prediction/#features","text":"A TimeSeriesPrediction sample yaml looks like below: apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef defines the reference to the kubernetes object including Node or other workload such as Deployment. spec.predictionMetrics defines the metrics about the spec.targetRef. spec.predictionWindowSeconds is a prediction time series duration. the TimeSeriesPredictionController will rotate the predicted data in spec.Status for consumer to consume the predicted time series data.","title":"Features"},{"location":"tutorials/using-time-series-prediction/#predictionmetrics","text":"apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\"","title":"PredictionMetrics"},{"location":"tutorials/using-time-series-prediction/#metrictype","text":"There are three types of the metric query: ResourceQuery is a kubernetes built-in resource metric such as cpu or memory. crane supports only cpu and memory now. RawQuery is a query by DSL, such as prometheus query language. now support prometheus. ExpressionQuery is a query by Expression selector. Now we only support prometheus as data source. We define the MetricType to orthogonal with the datasource. but now maybe some datasources do not support the metricType.","title":"MetricType"},{"location":"tutorials/using-time-series-prediction/#algorithm","text":"Algorithm define the algorithm type and params to do predict for the metric. Now there are two kinds of algorithms: dsp is an algorithm to forcasting a time series, it is based on FFT(Fast Fourier Transform), it is good at predicting some time series with seasonality and periods. percentile is an algorithm to estimate a time series, and find a recommended value to represent the past time series, it is based on exponentially-decaying weights historgram statistics. it is used to estimate a time series, it is not good at to predict a time sequences, although the percentile can output a time series predicted data, but it is all the same value. so if you want to predict a time sequences, dsp is a better choice.","title":"Algorithm"},{"location":"tutorials/using-time-series-prediction/#dsp-params","text":"","title":"dsp params"},{"location":"tutorials/using-time-series-prediction/#percentile-params","text":"","title":"percentile params"},{"location":"zh/","text":"\u4ecb\u7ecd \u00b6 The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling Features \u00b6 Time Series Prediction \u00b6 TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more. Effective HorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more. Analytics \u00b6 Analytics model analyzes the workload and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas. QoS Ensurance \u00b6 Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more. \u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6 \u00b6 \u539f\u751f\u7684 Kubernetes \u8c03\u5ea6\u5668\u53ea\u80fd\u57fa\u4e8e\u8d44\u6e90\u7684 Request \u8fdb\u884c\u8c03\u5ea6\u4e1a\u52a1\uff0c\u8fd9\u5f88\u5bb9\u6613\u5bfc\u81f4\u96c6\u7fa4\u8d1f\u8f7d\u4e0d\u5747\u7684\u95ee\u9898\u3002\u4e0e\u4e4b\u5bf9\u6bd4\u7684\u662f\uff0c Crane-scheudler \u53ef\u4ee5\u76f4\u63a5\u4ece Prometheus \u83b7\u53d6\u8282\u70b9\u7684\u771f\u5b9e\u8d1f\u8f7d\u60c5\u51b5\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8c03\u5ea6\u3002 \u66f4\u591a\u8bf7\u53c2\u89c1 \u6587\u6863 \u3002 Repositories \u00b6 Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - \u4e00\u4e2a\u53ef\u4ee5\u57fa\u4e8e\u771f\u5b9e\u8d1f\u8f7d\u5bf9\u4e1a\u52a1\u8fdb\u884c\u8c03\u5ea6\u7684 Kubernestes \u8c03\u5ea6\u5668\u3002","title":"\u4ecb\u7ecd"},{"location":"zh/#_1","text":"The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling","title":"\u4ecb\u7ecd"},{"location":"zh/#features","text":"","title":"Features"},{"location":"zh/#time-series-prediction","text":"TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more.","title":"Time Series Prediction"},{"location":"zh/#effective-horizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more.","title":"Effective HorizontalPodAutoscaler"},{"location":"zh/#analytics","text":"Analytics model analyzes the workload and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas.","title":"Analytics"},{"location":"zh/#qos-ensurance","text":"Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more.","title":"QoS Ensurance"},{"location":"zh/#_2","text":"\u539f\u751f\u7684 Kubernetes \u8c03\u5ea6\u5668\u53ea\u80fd\u57fa\u4e8e\u8d44\u6e90\u7684 Request \u8fdb\u884c\u8c03\u5ea6\u4e1a\u52a1\uff0c\u8fd9\u5f88\u5bb9\u6613\u5bfc\u81f4\u96c6\u7fa4\u8d1f\u8f7d\u4e0d\u5747\u7684\u95ee\u9898\u3002\u4e0e\u4e4b\u5bf9\u6bd4\u7684\u662f\uff0c Crane-scheudler \u53ef\u4ee5\u76f4\u63a5\u4ece Prometheus \u83b7\u53d6\u8282\u70b9\u7684\u771f\u5b9e\u8d1f\u8f7d\u60c5\u51b5\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8c03\u5ea6\u3002 \u66f4\u591a\u8bf7\u53c2\u89c1 \u6587\u6863 \u3002","title":"\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6"},{"location":"zh/#repositories","text":"Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - \u4e00\u4e2a\u53ef\u4ee5\u57fa\u4e8e\u771f\u5b9e\u8d1f\u8f7d\u5bf9\u4e1a\u52a1\u8fdb\u884c\u8c03\u5ea6\u7684 Kubernestes \u8c03\u5ea6\u5668\u3002","title":"Repositories"},{"location":"zh/CONTRIBUTING/","text":"Contributing to Crane \u00b6 Welcome to Crane! This document is a guideline about how to contribute to Crane. Become a contributor \u00b6 You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides . Report bugs \u00b6 Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug. Suggest enhancements \u00b6 If you have an idea to improve Crane, submit an feature request .","title":"\u8d21\u732e"},{"location":"zh/CONTRIBUTING/#contributing-to-crane","text":"Welcome to Crane! This document is a guideline about how to contribute to Crane.","title":"Contributing to Crane"},{"location":"zh/CONTRIBUTING/#become-a-contributor","text":"You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides .","title":"Become a contributor"},{"location":"zh/CONTRIBUTING/#report-bugs","text":"Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug.","title":"Report bugs"},{"location":"zh/CONTRIBUTING/#suggest-enhancements","text":"If you have an idea to improve Crane, submit an feature request .","title":"Suggest enhancements"},{"location":"zh/code-standards/","text":"Code standards \u00b6 This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project import need to be organized \u00b6 import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" ) logs standard \u00b6 logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err )) event is needed for critical reconcile loop \u00b6 event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ()) comment \u00b6 every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) } functions \u00b6 function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils variable \u00b6 variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop folder and file \u00b6 folder name should be letter with lower case and number file name should be letter and number and _ unit test \u00b6 Test-driven developing Complex function that include condition decide should add unit test for it don't forget to run make fmt before you submit code \u00b6","title":"\u4ee3\u7801\u6807\u51c6"},{"location":"zh/code-standards/#code-standards","text":"This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project","title":"Code standards"},{"location":"zh/code-standards/#import-need-to-be-organized","text":"import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" )","title":"import need to be organized"},{"location":"zh/code-standards/#logs-standard","text":"logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err ))","title":"logs standard"},{"location":"zh/code-standards/#event-is-needed-for-critical-reconcile-loop","text":"event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ())","title":"event is needed for critical reconcile loop"},{"location":"zh/code-standards/#comment","text":"every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) }","title":"comment"},{"location":"zh/code-standards/#functions","text":"function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils","title":"functions"},{"location":"zh/code-standards/#variable","text":"variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop","title":"variable"},{"location":"zh/code-standards/#folder-and-file","text":"folder name should be letter with lower case and number file name should be letter and number and _","title":"folder and file"},{"location":"zh/code-standards/#unit-test","text":"Test-driven developing Complex function that include condition decide should add unit test for it","title":"unit test"},{"location":"zh/code-standards/#dont-forget-to-run-make-fmt-before-you-submit-code","text":"","title":"don't forget to run make fmt before you submit code"},{"location":"zh/installation/","text":"\u4ea7\u54c1\u90e8\u7f72\u6307\u5357 \u00b6 \u4e3a\u4e86\u8ba9\u60a8\u66f4\u5feb\u7684\u90e8\u7f72 Crane \uff0c\u672c\u6587\u6863\u63d0\u4f9b\u6e05\u6670\u7684\uff1a \u90e8\u7f72\u73af\u5883\u8981\u6c42 \u5177\u4f53\u5b89\u88c5\u6b65\u9aa4 Crane \u5b89\u88c5\u65f6\u95f4\u572810\u5206\u949f\u5de6\u53f3\uff0c\u5177\u4f53\u65f6\u95f4\u4e5f\u4f9d\u8d56\u96c6\u7fa4\u89c4\u6a21\u4ee5\u53ca\u786c\u4ef6\u80fd\u529b\u3002\u76ee\u524d\u5b89\u88c5\u5df2\u7ecf\u975e\u5e38\u6210\u719f\uff0c\u5982\u679c\u60a8\u5b89\u88c5\u4e2d\u9047\u5230\u4efb\u4f55\u95ee\u9898\uff0c\u53ef\u4ee5\u91c7\u53d6\u5982\u4e0b\u51e0\u79cd\u65b9\u5f0f\uff1a \u8bf7\u9996\u5148\u68c0\u67e5\u540e\u6587\u7684 F&Q \u53ef\u4ee5\u63d0\u51fa\u4e00\u4e2a Issue \uff0c\u6211\u4eec\u4f1a\u8ba4\u771f\u5bf9\u5f85\u6bcf\u4e00\u4e2a Issue \u90e8\u7f72\u73af\u5883\u8981\u6c42 \u00b6 Kubernetes 1.18+ Helm 3.1.0 \u5b89\u88c5\u6d41\u7a0b \u00b6 \u5b89\u88c5 Helm \u00b6 \u5efa\u8bae\u53c2\u8003 Helm \u5b98\u7f51 \u5b89\u88c5\u6587\u6863 \u3002 \u5b89\u88c5 Prometheus \u548c Grafana \u00b6 \u4f7f\u7528 Helm \u5b89\u88c5 Prometheus \u548c Grafana\u3002 \u6ce8\u610f \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u73af\u5883\u4e2d\u90e8\u7f72\u4e86 Prometheus \u548c Grafana\uff0c\u53ef\u4ee5\u8df3\u8fc7\u8be5\u6b65\u9aa4\u3002 \u7f51\u7edc\u95ee\u9898 \u5982\u679c\u4f60\u7684\u7f51\u7edc\u65e0\u6cd5\u8bbf\u95eeGitHub\u8d44\u6e90(GitHub Release, GitHub Raw Content raw.githubusercontent.com )\u3002 \u90a3\u4e48\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u955c\u50cf\u4ed3\u5e93\u3002\u4f46\u955c\u50cf\u4ed3\u5e93\u5177\u6709\u4e00\u5b9a\u7684 \u65f6\u5ef6 \u3002 \u955c\u50cf\u4ed3\u5e93 Crane \u4f7f\u7528 Prometheus \u6293\u53d6\u96c6\u7fa4\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u8d44\u6e90\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u5b89\u88c5 Prometheus\uff1a Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Crane \u7684 Fadvisor \u4f7f\u7528 Grafana \u5c55\u793a\u6210\u672c\u9884\u4f30\u3002\u5b89\u88c5 Grafana\uff1a Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana \u5b89\u88c5 Crane \u548c Fadvisor \u00b6 Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor \u5b89\u88c5 Crane-scheduler\uff08\u53ef\u9009\uff09 \u00b6 helm install scheduler -n crane-system --create-namespace crane/scheduler \u9a8c\u8bc1\u5b89\u88c5\u662f\u5426\u6210\u529f \u00b6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684 Deployment \u662f\u5426\u6b63\u5e38\uff1a kubectl get deploy -n crane-system \u7ed3\u679c\u7c7b\u4f3c\u5982\u4e0b\uff1a NAME READY UP-TO-DATE AVAILABLE AGE craned 1 /1 1 1 31m fadvisor 1 /1 1 1 41m grafana 1 /1 1 1 42m metric-adapter 1 /1 1 1 31m prometheus-kube-state-metrics 1 /1 1 1 43m prometheus-server 1 /1 1 1 43m \u53ef\u4ee5\u67e5\u770b\u672c\u7bc7 \u6587\u6863 \u83b7\u53d6\u66f4\u591a\u6709\u5173 Crane Helm Chart \u7684\u4fe1\u606f\u3002 \u6210\u672c\u5c55\u793a \u00b6 \u6253\u5f00 Crane \u63a7\u5236\u53f0 \u00b6 \u6ce8\u610f\uff1aCrane \u7684\u63a7\u5236\u53f0\u5730\u5740\u5c31\u662f Crane \u7684 URL \u5730\u5740\uff0c\u53ef\u4ee5\u5c06\u5176\u6dfb\u52a0\u5230\u7edf\u4e00\u7684\u63a7\u5236\u53f0\u67e5\u770b\u591a\u4e2a\u90e8\u7f72 Crane \u7684\u96c6\u7fa4\u7684\u4fe1\u606f\u3002 \u5229\u7528 Port forwarding \u547d\u4ee4\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u6253\u5f00 Crane \u63a7\u5236\u53f0\uff1a kubectl port-forward -n crane-system svc/craned 9090 \u6267\u884c\u4e0a\u8ff0\u547d\u4ee4\u540e\uff0c\u4e0d\u8981\u5173\u95ed\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u5730\u5740\u91cc\u8f93\u5165 localhost:9090 \u5373\u53ef\u6253\u5f00 Crane \u7684\u63a7\u5236\u53f0\uff1a \u6dfb\u52a0\u5b89\u88c5\u4e86 Crane \u7684\u96c6\u7fa4 \u00b6 \u60a8\u53ef\u4ee5\u70b9\u51fb\u4e0a\u56fe\u4e2d\u7684\u201c\u6dfb\u52a0\u96c6\u7fa4\u201d\u7684\u84dd\u8272\u6309\u94ae\uff0c\u5c06 Crane \u63a7\u5236\u53f0\u7684\u5730\u5740 http://localhost:9090 \u4f5c\u4e3a Crane \u7684 URL\uff0c\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u96c6\u7fa4\u6dfb\u52a0\u5230 Crane \u63a7\u5236\u53f0\u3002 \u82e5\u60a8\u60f3\u6dfb\u52a0\u5176\u5b83\u96c6\u7fa4\uff0c\u5b9e\u73b0\u591a\u96c6\u7fa4\u7684\u8d44\u6e90\u4f7f\u7528\u548c\u6210\u672c\u5206\u6790\u3002\u53ef\u4ee5\u5728\u522b\u7684\u96c6\u7fa4\u4e2d\u4e5f\u5b89\u88c5\u5b8c Crane \u4e4b\u540e\uff0c\u5c06 Crane \u7684 URL \u6dfb\u52a0\u8fdb\u6765\u3002 \u81ea\u5b9a\u4e49\u5b89\u88c5 \u00b6 \u901a\u8fc7 YAML \u5b89\u88c5 Crane \u3002 Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter \u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49 Crane \u91cc\u914d\u7f6e Prometheus \u7684 HTTP \u5730\u5740\uff0c\u8bf7\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u3002\u5982\u679c\u60a8\u5728\u96c6\u7fa4\u91cc\u5df2\u5b58\u5728\u4e00\u4e2a Prometheus\uff0c\u8bf7\u5c06 Server \u5730\u5740\u586b\u4e8e CUSTOMIZE_PROMETHEUS \u3002 export CUSTOMIZE_PROMETHEUS= if [ $CUSTOMIZE_PROMETHEUS ]; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/${CUSTOMIZE_PROMETHEUS}/\" deploy/craned/deployment.yaml ; fi \u5b89\u88c5\u5e38\u89c1\u95ee\u9898 \u00b6 \u5b89\u88c5 Crane \u62a5\u9519 \u00b6 \u5f53\u60a8\u6267\u884c helm install crane -n crane-system --create-namespace crane/crane \u547d\u4ee4\u65f6\uff0c\u53ef\u80fd\u4f1a\u9047\u5230\u5982\u4e0b\u9519\u8bef\uff1a Error: rendered manifests contain a resource that already exists. Unable to continue with install: APIService \"v1beta1.custom.metrics.k8s.io\" in namespace \"\" exists and cannot be imported into the current release: invalid ownership metadata ; label validation error: missing key \"app.kubernetes.io/managed-by\" : must be set to \"Helm\" ; annotation validation error: missing key \"meta.helm.sh/release-name\" : must be set to \"crane\" ; annotation validation error: missing key \"meta.helm.sh/release-namespace\" : must be set to \"crane-system\" \u539f\u56e0\uff1a\u96c6\u7fa4\u5b89\u88c5\u8fc7 custom metric \u7684 APIService\uff0c\u6240\u4ee5\u62a5\u9519\u3002\u53ef\u4ee5\u628a\u4e4b\u524d\u7684\u5220\u9664\u518d\u91cd\u65b0\u6267\u884c\u5b89\u88c5 Crane \u7684\u547d\u4ee4\uff0c\u5220\u9664\u65b9\u5f0f\uff1a kubectl delete apiservice v1beta1.custom.metrics.k8s.io \u3002 \u83b7\u53d6 Crane URL \u7684\u5176\u5b83\u65b9\u5f0f \u00b6 NodePort \u65b9\u5f0f \u00b6 \u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210 NodePort \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u96c6\u7fa4\u4efb\u610f\u8282\u70b9 IP + \u8be5\u670d\u52a1\u91ccdashboard- service \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a NodePort \u7684\u65b9\u5f0f\uff0c\u7136\u540e\u83b7\u53d6\u67d0\u4e00\u96c6\u7fa4\u7684\u8282\u70b9 IP\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u7aef\u53e3\u53f7\uff0c\u7aef\u53e3\u53f7\u5982\u4e0b\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u82e5\u60a8\u7684\u96c6\u7fa4\u8282\u70b9\u53ea\u6709\u5185\u7f51 IP\uff0c\u5219\u8bbf\u95ee\u8be5 IP \u7684\u8ba1\u7b97\u673a\u9700\u8981\u5728\u540c\u4e00\u5185\u7f51\u3002\u82e5\u96c6\u7fa4\u8282\u70b9\u62e5\u6709\u5916\u7f51 IP\uff0c\u5219\u6ca1\u6709\u76f8\u5173\u95ee\u9898\u3002 LoadBalance \u65b9\u5f0f \u00b6 \u82e5\u60a8\u4f7f\u7528\u7684\u662f\u516c\u6709\u4e91\u5382\u5546\u7684\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210\u516c\u7f51 LB \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7 LB IP + 9090 \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a\u516c\u7f51 LB \u7684\u65b9\u5f0f\u3002","title":"\u5b89\u88c5"},{"location":"zh/installation/#_1","text":"\u4e3a\u4e86\u8ba9\u60a8\u66f4\u5feb\u7684\u90e8\u7f72 Crane \uff0c\u672c\u6587\u6863\u63d0\u4f9b\u6e05\u6670\u7684\uff1a \u90e8\u7f72\u73af\u5883\u8981\u6c42 \u5177\u4f53\u5b89\u88c5\u6b65\u9aa4 Crane \u5b89\u88c5\u65f6\u95f4\u572810\u5206\u949f\u5de6\u53f3\uff0c\u5177\u4f53\u65f6\u95f4\u4e5f\u4f9d\u8d56\u96c6\u7fa4\u89c4\u6a21\u4ee5\u53ca\u786c\u4ef6\u80fd\u529b\u3002\u76ee\u524d\u5b89\u88c5\u5df2\u7ecf\u975e\u5e38\u6210\u719f\uff0c\u5982\u679c\u60a8\u5b89\u88c5\u4e2d\u9047\u5230\u4efb\u4f55\u95ee\u9898\uff0c\u53ef\u4ee5\u91c7\u53d6\u5982\u4e0b\u51e0\u79cd\u65b9\u5f0f\uff1a \u8bf7\u9996\u5148\u68c0\u67e5\u540e\u6587\u7684 F&Q \u53ef\u4ee5\u63d0\u51fa\u4e00\u4e2a Issue \uff0c\u6211\u4eec\u4f1a\u8ba4\u771f\u5bf9\u5f85\u6bcf\u4e00\u4e2a Issue","title":"\u4ea7\u54c1\u90e8\u7f72\u6307\u5357"},{"location":"zh/installation/#_2","text":"Kubernetes 1.18+ Helm 3.1.0","title":"\u90e8\u7f72\u73af\u5883\u8981\u6c42"},{"location":"zh/installation/#_3","text":"","title":"\u5b89\u88c5\u6d41\u7a0b"},{"location":"zh/installation/#helm","text":"\u5efa\u8bae\u53c2\u8003 Helm \u5b98\u7f51 \u5b89\u88c5\u6587\u6863 \u3002","title":"\u5b89\u88c5 Helm"},{"location":"zh/installation/#prometheus-grafana","text":"\u4f7f\u7528 Helm \u5b89\u88c5 Prometheus \u548c Grafana\u3002 \u6ce8\u610f \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u73af\u5883\u4e2d\u90e8\u7f72\u4e86 Prometheus \u548c Grafana\uff0c\u53ef\u4ee5\u8df3\u8fc7\u8be5\u6b65\u9aa4\u3002 \u7f51\u7edc\u95ee\u9898 \u5982\u679c\u4f60\u7684\u7f51\u7edc\u65e0\u6cd5\u8bbf\u95eeGitHub\u8d44\u6e90(GitHub Release, GitHub Raw Content raw.githubusercontent.com )\u3002 \u90a3\u4e48\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u955c\u50cf\u4ed3\u5e93\u3002\u4f46\u955c\u50cf\u4ed3\u5e93\u5177\u6709\u4e00\u5b9a\u7684 \u65f6\u5ef6 \u3002 \u955c\u50cf\u4ed3\u5e93 Crane \u4f7f\u7528 Prometheus \u6293\u53d6\u96c6\u7fa4\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u8d44\u6e90\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u5b89\u88c5 Prometheus\uff1a Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Crane \u7684 Fadvisor \u4f7f\u7528 Grafana \u5c55\u793a\u6210\u672c\u9884\u4f30\u3002\u5b89\u88c5 Grafana\uff1a Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana","title":"\u5b89\u88c5 Prometheus \u548c Grafana"},{"location":"zh/installation/#crane-fadvisor","text":"Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor","title":"\u5b89\u88c5 Crane \u548c Fadvisor"},{"location":"zh/installation/#crane-scheduler","text":"helm install scheduler -n crane-system --create-namespace crane/scheduler","title":"\u5b89\u88c5 Crane-scheduler\uff08\u53ef\u9009\uff09"},{"location":"zh/installation/#_4","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684 Deployment \u662f\u5426\u6b63\u5e38\uff1a kubectl get deploy -n crane-system \u7ed3\u679c\u7c7b\u4f3c\u5982\u4e0b\uff1a NAME READY UP-TO-DATE AVAILABLE AGE craned 1 /1 1 1 31m fadvisor 1 /1 1 1 41m grafana 1 /1 1 1 42m metric-adapter 1 /1 1 1 31m prometheus-kube-state-metrics 1 /1 1 1 43m prometheus-server 1 /1 1 1 43m \u53ef\u4ee5\u67e5\u770b\u672c\u7bc7 \u6587\u6863 \u83b7\u53d6\u66f4\u591a\u6709\u5173 Crane Helm Chart \u7684\u4fe1\u606f\u3002","title":"\u9a8c\u8bc1\u5b89\u88c5\u662f\u5426\u6210\u529f"},{"location":"zh/installation/#_5","text":"","title":"\u6210\u672c\u5c55\u793a"},{"location":"zh/installation/#crane","text":"\u6ce8\u610f\uff1aCrane \u7684\u63a7\u5236\u53f0\u5730\u5740\u5c31\u662f Crane \u7684 URL \u5730\u5740\uff0c\u53ef\u4ee5\u5c06\u5176\u6dfb\u52a0\u5230\u7edf\u4e00\u7684\u63a7\u5236\u53f0\u67e5\u770b\u591a\u4e2a\u90e8\u7f72 Crane \u7684\u96c6\u7fa4\u7684\u4fe1\u606f\u3002 \u5229\u7528 Port forwarding \u547d\u4ee4\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u6253\u5f00 Crane \u63a7\u5236\u53f0\uff1a kubectl port-forward -n crane-system svc/craned 9090 \u6267\u884c\u4e0a\u8ff0\u547d\u4ee4\u540e\uff0c\u4e0d\u8981\u5173\u95ed\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u5730\u5740\u91cc\u8f93\u5165 localhost:9090 \u5373\u53ef\u6253\u5f00 Crane \u7684\u63a7\u5236\u53f0\uff1a","title":"\u6253\u5f00 Crane \u63a7\u5236\u53f0"},{"location":"zh/installation/#crane_1","text":"\u60a8\u53ef\u4ee5\u70b9\u51fb\u4e0a\u56fe\u4e2d\u7684\u201c\u6dfb\u52a0\u96c6\u7fa4\u201d\u7684\u84dd\u8272\u6309\u94ae\uff0c\u5c06 Crane \u63a7\u5236\u53f0\u7684\u5730\u5740 http://localhost:9090 \u4f5c\u4e3a Crane \u7684 URL\uff0c\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u96c6\u7fa4\u6dfb\u52a0\u5230 Crane \u63a7\u5236\u53f0\u3002 \u82e5\u60a8\u60f3\u6dfb\u52a0\u5176\u5b83\u96c6\u7fa4\uff0c\u5b9e\u73b0\u591a\u96c6\u7fa4\u7684\u8d44\u6e90\u4f7f\u7528\u548c\u6210\u672c\u5206\u6790\u3002\u53ef\u4ee5\u5728\u522b\u7684\u96c6\u7fa4\u4e2d\u4e5f\u5b89\u88c5\u5b8c Crane \u4e4b\u540e\uff0c\u5c06 Crane \u7684 URL \u6dfb\u52a0\u8fdb\u6765\u3002","title":"\u6dfb\u52a0\u5b89\u88c5\u4e86 Crane \u7684\u96c6\u7fa4"},{"location":"zh/installation/#_6","text":"\u901a\u8fc7 YAML \u5b89\u88c5 Crane \u3002 Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter \u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49 Crane \u91cc\u914d\u7f6e Prometheus \u7684 HTTP \u5730\u5740\uff0c\u8bf7\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u3002\u5982\u679c\u60a8\u5728\u96c6\u7fa4\u91cc\u5df2\u5b58\u5728\u4e00\u4e2a Prometheus\uff0c\u8bf7\u5c06 Server \u5730\u5740\u586b\u4e8e CUSTOMIZE_PROMETHEUS \u3002 export CUSTOMIZE_PROMETHEUS= if [ $CUSTOMIZE_PROMETHEUS ]; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/${CUSTOMIZE_PROMETHEUS}/\" deploy/craned/deployment.yaml ; fi","title":"\u81ea\u5b9a\u4e49\u5b89\u88c5"},{"location":"zh/installation/#_7","text":"","title":"\u5b89\u88c5\u5e38\u89c1\u95ee\u9898"},{"location":"zh/installation/#crane_2","text":"\u5f53\u60a8\u6267\u884c helm install crane -n crane-system --create-namespace crane/crane \u547d\u4ee4\u65f6\uff0c\u53ef\u80fd\u4f1a\u9047\u5230\u5982\u4e0b\u9519\u8bef\uff1a Error: rendered manifests contain a resource that already exists. Unable to continue with install: APIService \"v1beta1.custom.metrics.k8s.io\" in namespace \"\" exists and cannot be imported into the current release: invalid ownership metadata ; label validation error: missing key \"app.kubernetes.io/managed-by\" : must be set to \"Helm\" ; annotation validation error: missing key \"meta.helm.sh/release-name\" : must be set to \"crane\" ; annotation validation error: missing key \"meta.helm.sh/release-namespace\" : must be set to \"crane-system\" \u539f\u56e0\uff1a\u96c6\u7fa4\u5b89\u88c5\u8fc7 custom metric \u7684 APIService\uff0c\u6240\u4ee5\u62a5\u9519\u3002\u53ef\u4ee5\u628a\u4e4b\u524d\u7684\u5220\u9664\u518d\u91cd\u65b0\u6267\u884c\u5b89\u88c5 Crane \u7684\u547d\u4ee4\uff0c\u5220\u9664\u65b9\u5f0f\uff1a kubectl delete apiservice v1beta1.custom.metrics.k8s.io \u3002","title":"\u5b89\u88c5 Crane \u62a5\u9519"},{"location":"zh/installation/#crane-url","text":"","title":"\u83b7\u53d6 Crane URL \u7684\u5176\u5b83\u65b9\u5f0f"},{"location":"zh/installation/#nodeport","text":"\u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210 NodePort \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u96c6\u7fa4\u4efb\u610f\u8282\u70b9 IP + \u8be5\u670d\u52a1\u91ccdashboard- service \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a NodePort \u7684\u65b9\u5f0f\uff0c\u7136\u540e\u83b7\u53d6\u67d0\u4e00\u96c6\u7fa4\u7684\u8282\u70b9 IP\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u7aef\u53e3\u53f7\uff0c\u7aef\u53e3\u53f7\u5982\u4e0b\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u82e5\u60a8\u7684\u96c6\u7fa4\u8282\u70b9\u53ea\u6709\u5185\u7f51 IP\uff0c\u5219\u8bbf\u95ee\u8be5 IP \u7684\u8ba1\u7b97\u673a\u9700\u8981\u5728\u540c\u4e00\u5185\u7f51\u3002\u82e5\u96c6\u7fa4\u8282\u70b9\u62e5\u6709\u5916\u7f51 IP\uff0c\u5219\u6ca1\u6709\u76f8\u5173\u95ee\u9898\u3002","title":"NodePort \u65b9\u5f0f"},{"location":"zh/installation/#loadbalance","text":"\u82e5\u60a8\u4f7f\u7528\u7684\u662f\u516c\u6709\u4e91\u5382\u5546\u7684\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210\u516c\u7f51 LB \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7 LB IP + 9090 \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a\u516c\u7f51 LB \u7684\u65b9\u5f0f\u3002","title":"LoadBalance \u65b9\u5f0f"},{"location":"zh/mirror/","text":"\u955c\u50cf\u4ed3\u5e93 \u00b6 \u5173\u4e8e\u955c\u50cf\u4ed3\u5e93 \u00b6 \u56e0\u4e3a\u5404\u79cd\u7f51\u7edc\u95ee\u9898\uff0c\u5bfc\u81f4\u90e8\u5206\u5730\u57df\u96be\u4ee5\u8bbf\u95eeGitHub \u8d44\u6e90\uff0c\u5982GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com \u3002 \u4e3a\u4e86\u66f4\u597d\u7684\u4f7f\u7528\u4f53\u9a8c\uff0cGoCrane \u4e3a\u60a8\u989d\u5916\u63d0\u4f9b\u4e86\u591a\u4e2a\u955c\u50cf\u4ed3\u5e93\uff0c\u4f46\u5177\u6709\u4e00\u5b9a\u7684\u65f6\u5ef6\u3002 Helm Resources \u00b6 Tips \u6bcf\u516d\u5c0f\u65f6\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u7684\u6700\u65b0\u7248\u672c Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public Git Resources \u00b6 Tips \u6bcf\u5929\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u4ed3\u5e93 Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public \u83b7\u53d6 Coding Git \u4ed3\u5e93\u6e90\u6587\u4ef6\u5185\u5bb9 \u00b6 \u5728\u8fd9\u91cc\u5c06\u4e3a\u60a8\u4ecb\u7ecd\uff0c\u5982\u4f55\u901a\u8fc7HTTP\u8bf7\u6c42\u76f4\u63a5\u83b7\u53d6 Coding Git \u4ed3\u5e93\u4e2d\u7684\u6e90\u6587\u4ef6\u5185\u5bb9\u3002 Coding Git \u4ed3\u5e93\u7684\u5173\u952e\u53c2\u6570 \u00b6 \u4e0e\u5e38\u89c4\u7684API\u8bf7\u6c42\u7c7b\u4f3c\uff0cCoding Git\u4ed3\u5e93\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684API\u63a5\u53e3\u3002 \u4e0b\u9762\u4e3a\u60a8\u4ecb\u7ecd\u76f8\u5173\u7684\u53c2\u6570\u3002 Example \u4ee5 https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml \u4f5c\u4e3a\u4f8b\u5b50\u3002 \u70b9\u51fb\u8bbf\u95ee \u53c2\u6570 \u8bf4\u660e \u4f8b\u5b50 team \u56e2\u961f\u540d\u79f0 finops project \u9879\u76ee\u540d\u79f0 gocrane repo Git \u4ed3\u5e93\u540d\u79f0 helm-charts branch \u5206\u652f\u540d\u79f0 main file path \u9879\u76ee\u4e2d\u7684\u6587\u4ef6\u8def\u5f84 /integration/grafana/override_values.yaml \u6784\u9020HTTP\u8bf7\u6c42 \u00b6 \u6839\u636e\u4e0a\u9762\u6240\u63d0\u5230\u7684\u5c5e\u6027\uff0c\u6309\u7167\u4e0b\u9762\u7684URL\u6784\u9020\u89c4\u5219\u4f9d\u6b21\u586b\u5165\uff0c\u5373\u53ef\u83b7\u5f97\u4e00\u4e2a\u53ef\u4ee5\u76f4\u63a5\u83b7\u53d6\u6e90\u6587\u4ef6\u5185\u5bb9\u7684URL\u3002 https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips \u5c1d\u8bd5\u4ee5\u4e0b\u7684\u547d\u4ee4 curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"\u955c\u50cf\u4ed3\u5e93"},{"location":"zh/mirror/#_1","text":"","title":"\u955c\u50cf\u4ed3\u5e93"},{"location":"zh/mirror/#_2","text":"\u56e0\u4e3a\u5404\u79cd\u7f51\u7edc\u95ee\u9898\uff0c\u5bfc\u81f4\u90e8\u5206\u5730\u57df\u96be\u4ee5\u8bbf\u95eeGitHub \u8d44\u6e90\uff0c\u5982GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com \u3002 \u4e3a\u4e86\u66f4\u597d\u7684\u4f7f\u7528\u4f53\u9a8c\uff0cGoCrane \u4e3a\u60a8\u989d\u5916\u63d0\u4f9b\u4e86\u591a\u4e2a\u955c\u50cf\u4ed3\u5e93\uff0c\u4f46\u5177\u6709\u4e00\u5b9a\u7684\u65f6\u5ef6\u3002","title":"\u5173\u4e8e\u955c\u50cf\u4ed3\u5e93"},{"location":"zh/mirror/#helm-resources","text":"Tips \u6bcf\u516d\u5c0f\u65f6\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u7684\u6700\u65b0\u7248\u672c Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public","title":"Helm Resources"},{"location":"zh/mirror/#git-resources","text":"Tips \u6bcf\u5929\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u4ed3\u5e93 Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public","title":"Git Resources"},{"location":"zh/mirror/#coding-git","text":"\u5728\u8fd9\u91cc\u5c06\u4e3a\u60a8\u4ecb\u7ecd\uff0c\u5982\u4f55\u901a\u8fc7HTTP\u8bf7\u6c42\u76f4\u63a5\u83b7\u53d6 Coding Git \u4ed3\u5e93\u4e2d\u7684\u6e90\u6587\u4ef6\u5185\u5bb9\u3002","title":"\u83b7\u53d6 Coding Git \u4ed3\u5e93\u6e90\u6587\u4ef6\u5185\u5bb9"},{"location":"zh/mirror/#coding-git_1","text":"\u4e0e\u5e38\u89c4\u7684API\u8bf7\u6c42\u7c7b\u4f3c\uff0cCoding Git\u4ed3\u5e93\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684API\u63a5\u53e3\u3002 \u4e0b\u9762\u4e3a\u60a8\u4ecb\u7ecd\u76f8\u5173\u7684\u53c2\u6570\u3002 Example \u4ee5 https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml \u4f5c\u4e3a\u4f8b\u5b50\u3002 \u70b9\u51fb\u8bbf\u95ee \u53c2\u6570 \u8bf4\u660e \u4f8b\u5b50 team \u56e2\u961f\u540d\u79f0 finops project \u9879\u76ee\u540d\u79f0 gocrane repo Git \u4ed3\u5e93\u540d\u79f0 helm-charts branch \u5206\u652f\u540d\u79f0 main file path \u9879\u76ee\u4e2d\u7684\u6587\u4ef6\u8def\u5f84 /integration/grafana/override_values.yaml","title":"Coding Git \u4ed3\u5e93\u7684\u5173\u952e\u53c2\u6570"},{"location":"zh/mirror/#http","text":"\u6839\u636e\u4e0a\u9762\u6240\u63d0\u5230\u7684\u5c5e\u6027\uff0c\u6309\u7167\u4e0b\u9762\u7684URL\u6784\u9020\u89c4\u5219\u4f9d\u6b21\u586b\u5165\uff0c\u5373\u53ef\u83b7\u5f97\u4e00\u4e2a\u53ef\u4ee5\u76f4\u63a5\u83b7\u53d6\u6e90\u6587\u4ef6\u5185\u5bb9\u7684URL\u3002 https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips \u5c1d\u8bd5\u4ee5\u4e0b\u7684\u547d\u4ee4 curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"\u6784\u9020HTTP\u8bf7\u6c42"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/","text":"Advanced CPUSet Manager \u00b6 Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet. Table of Contents \u00b6 Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations Motivation \u00b6 Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service. Goals \u00b6 Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus Non-Goals/Future Work \u00b6 Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered Proposal \u00b6 Relax restrictions of cpuset allocation \u00b6 Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2 Add new annotation to describe the requirement of cpuset contorl manger \u00b6 apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use. Advanced CPU Manager component \u00b6 Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet User Stories \u00b6 Users can update pod annotaion to control cpuset policy flexibly Story 1 \u00b6 make pod from none to share without recreating pod Story 2 \u00b6 make pod from exclusive to share, so offline process can use these CPUs Risks and Mitigations \u00b6 kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Advanced CpuSet Manager"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#advanced-cpuset-manager","text":"Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet.","title":"Advanced CPUSet Manager"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#table-of-contents","text":"Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations","title":"Table of Contents"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#motivation","text":"Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service.","title":"Motivation"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#goals","text":"Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus","title":"Goals"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#non-goalsfuture-work","text":"Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered","title":"Non-Goals/Future Work"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#proposal","text":"","title":"Proposal"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#relax-restrictions-of-cpuset-allocation","text":"Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2","title":"Relax restrictions of cpuset allocation"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#add-new-annotation-to-describe-the-requirement-of-cpuset-contorl-manger","text":"apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use.","title":"Add new annotation to describe the  requirement of cpuset contorl manger"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#advanced-cpu-manager-component","text":"Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet","title":"Advanced CPU Manager component"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#user-stories","text":"Users can update pod annotaion to control cpuset policy flexibly","title":"User Stories"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#story-1","text":"make pod from none to share without recreating pod","title":"Story 1"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#story-2","text":"make pod from exclusive to share, so offline process can use these CPUs","title":"Story 2"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#risks-and-mitigations","text":"kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Risks and Mitigations"},{"location":"zh/roadmaps/roadmap-1h-2022/","text":"Crane Roadmap for H1 2022 \u00b6 Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan. 0.1.0 [released] \u00b6 Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing 0.2.0\uff1a[released] \u00b6 Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio 0.3.0 [released] \u00b6 UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA 0.4.0 [April] \u00b6 EVPA support Dynamic Scheduler UI to support EPA. 0.5.0 [May] \u00b6 HPC open source Node & Pod QoS Ensurance for DiskIO and Network Prediction with DiskIO, Network 0.6.0 [June] \u00b6 Scalability to support 3k TSP and 3k EPA Application Portrait SLO based Application QoS for CPU and Mem","title":"1H 2022"},{"location":"zh/roadmaps/roadmap-1h-2022/#crane-roadmap-for-h1-2022","text":"Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan.","title":"Crane Roadmap for H1 2022"},{"location":"zh/roadmaps/roadmap-1h-2022/#010-released","text":"Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing","title":"0.1.0 [released]"},{"location":"zh/roadmaps/roadmap-1h-2022/#020released","text":"Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio","title":"0.2.0\uff1a[released]"},{"location":"zh/roadmaps/roadmap-1h-2022/#030-released","text":"UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA","title":"0.3.0 [released]"},{"location":"zh/roadmaps/roadmap-1h-2022/#040-april","text":"EVPA support Dynamic Scheduler UI to support EPA.","title":"0.4.0 [April]"},{"location":"zh/roadmaps/roadmap-1h-2022/#050-may","text":"HPC open source Node & Pod QoS Ensurance for DiskIO and Network Prediction with DiskIO, Network","title":"0.5.0 [May]"},{"location":"zh/roadmaps/roadmap-1h-2022/#060-june","text":"Scalability to support 3k TSP and 3k EPA Application Portrait SLO based Application QoS for CPU and Mem","title":"0.6.0 [June]"},{"location":"zh/tutorials/analytics-and-recommendation/","text":"Analytics and Recommendation \u00b6 Analytics and Recommendation provide capacity that analyzes the workload in k8s cluster and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas. Analytics and Recommend Pod Resources \u00b6 Create an Resource Analytics to give recommendation for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics -n crane-system analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-resource 15m metric-adapter-resource 15m You can get created recommendation from analytics status: kubectl get analytics craned-resource -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : Resource status : lastSuccessfulTime : \"2022-01-12T08:40:59Z\" recommendations : - name : craned-resource-resource-j7shb namespace : crane-system uid : 8ce2eedc-7969-4b80-8aee-fd4a98d6a8b6 The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-j7shb -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-resource-resource-j7shb namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-resource uid : a9e6dc0d-ab26-4f2a-84bd-4fe9e0f3e105 spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : Resource status : conditions : - lastTransitionTime : \"2022-01-12T08:40:59Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastSuccessfulTime : \"2022-01-12T08:40:59Z\" lastUpdateTime : \"2022-01-12T08:40:59Z\" resourceRequest : containers : - containerName : craned target : cpu : 114m memory : 120586239m The status.resourceRequest is recommended by crane's recommendation engine. Something you should know about Resource recommendation: Resource Recommendation use historic prometheus metrics to calculate and propose. We use Percentile algorithm to process metrics that also used by VPA. If the workload is running for a long term like several weeks, the result will be more accurate. Analytics and Recommend HPA \u00b6 Create an HPA Analytics to give recommendations for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-hpa.yaml kubectl get analytics -n crane-system analytics-hpa.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-hpa 5m52s craned-resource 18h metric-adapter-hpa 5m52s metric-adapter-resource 18h You can get created recommendation from analytics status: kubectl get analytics craned-hpa -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : HPA status : lastSuccessfulTime : \"2022-01-13T07:26:18Z\" recommendations : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation name : craned-hpa-hpa-2f22w namespace : crane-system uid : 397733ee-986a-4630-af75-736d2b58bfac The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-2f22w -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-hpa-hpa-2f22w namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-hpa uid : b216d9c3-c52e-4c9c-b9e9-9d5b45165b1d spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : HPA status : conditions : - lastTransitionTime : \"2022-01-13T07:51:18Z\" message : 'Failed to offer recommend, Recommendation crane-system/craned-hpa-hpa-2f22w error EHPAAdvisor prediction metrics data is unexpected, List length is 0 ' reason : FailedOfferRecommend status : \"False\" type : Ready lastUpdateTime : \"2022-01-13T07:51:18Z\" The status.resourceRequest is recommended by crane's recommendation engine. The fail reason is demo workload don't have enough run time. Something you should know about HPA recommendation: HPA Recommendation use historic prometheus metrics to calculate, forecast and propose. We use DSP algorithm to process metrics. We recommend using Effective HorizontalPodAutoscaler to execute autoscaling, you can see this document to learn more. The Workload need match following conditions: Existing at least one ready pod Ready pod ratio should larger that 50% Must provide cpu request for pod spec The workload should be running for at least a week to get enough metrics to forecast The workload's cpu load should be predictable, too low or too unstable workload often is unpredictable","title":"Analytics and Recommendation"},{"location":"zh/tutorials/analytics-and-recommendation/#analytics-and-recommendation","text":"Analytics and Recommendation provide capacity that analyzes the workload in k8s cluster and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Recommend container requests & limit resources based on historic metrics. Effective HPARecommend : Recommend which workloads are suitable for autoscaling and provide optimized configurations such as minReplicas, maxReplicas.","title":"Analytics and Recommendation"},{"location":"zh/tutorials/analytics-and-recommendation/#analytics-and-recommend-pod-resources","text":"Create an Resource Analytics to give recommendation for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics -n crane-system analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-resource namespace : crane-system spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-resource 15m metric-adapter-resource 15m You can get created recommendation from analytics status: kubectl get analytics craned-resource -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-resource namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : Resource status : lastSuccessfulTime : \"2022-01-12T08:40:59Z\" recommendations : - name : craned-resource-resource-j7shb namespace : crane-system uid : 8ce2eedc-7969-4b80-8aee-fd4a98d6a8b6 The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-j7shb -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-resource-resource-j7shb namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-resource uid : a9e6dc0d-ab26-4f2a-84bd-4fe9e0f3e105 spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : Resource status : conditions : - lastTransitionTime : \"2022-01-12T08:40:59Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastSuccessfulTime : \"2022-01-12T08:40:59Z\" lastUpdateTime : \"2022-01-12T08:40:59Z\" resourceRequest : containers : - containerName : craned target : cpu : 114m memory : 120586239m The status.resourceRequest is recommended by crane's recommendation engine. Something you should know about Resource recommendation: Resource Recommendation use historic prometheus metrics to calculate and propose. We use Percentile algorithm to process metrics that also used by VPA. If the workload is running for a long term like several weeks, the result will be more accurate.","title":"Analytics and Recommend Pod Resources"},{"location":"zh/tutorials/analytics-and-recommendation/#analytics-and-recommend-hpa","text":"Create an HPA Analytics to give recommendations for deployment: craned and metric-adapter as a sample. kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-hpa.yaml kubectl get analytics -n crane-system analytics-hpa.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : craned --- apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : metric-adapter-hpa namespace : crane-system spec : type : HPA # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 3600 # analytics selected resources every 1 hour resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : metric-adapter The output is: NAME AGE craned-hpa 5m52s craned-resource 18h metric-adapter-hpa 5m52s metric-adapter-resource 18h You can get created recommendation from analytics status: kubectl get analytics craned-hpa -n crane-system -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : craned-hpa namespace : crane-system spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : craned type : HPA status : lastSuccessfulTime : \"2022-01-13T07:26:18Z\" recommendations : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation name : craned-hpa-hpa-2f22w namespace : crane-system uid : 397733ee-986a-4630-af75-736d2b58bfac The recommendation name presents on status.recommendations[0].name . Then you can get recommendation detail by running: kubectl get recommend -n crane-system craned-resource-resource-2f22w -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : name : craned-hpa-hpa-2f22w namespace : crane-system ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : craned-hpa uid : b216d9c3-c52e-4c9c-b9e9-9d5b45165b1d spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 targetRef : apiVersion : apps/v1 kind : Deployment name : craned namespace : crane-system type : HPA status : conditions : - lastTransitionTime : \"2022-01-13T07:51:18Z\" message : 'Failed to offer recommend, Recommendation crane-system/craned-hpa-hpa-2f22w error EHPAAdvisor prediction metrics data is unexpected, List length is 0 ' reason : FailedOfferRecommend status : \"False\" type : Ready lastUpdateTime : \"2022-01-13T07:51:18Z\" The status.resourceRequest is recommended by crane's recommendation engine. The fail reason is demo workload don't have enough run time. Something you should know about HPA recommendation: HPA Recommendation use historic prometheus metrics to calculate, forecast and propose. We use DSP algorithm to process metrics. We recommend using Effective HorizontalPodAutoscaler to execute autoscaling, you can see this document to learn more. The Workload need match following conditions: Existing at least one ready pod Ready pod ratio should larger that 50% Must provide cpu request for pod spec The workload should be running for at least a week to get enough metrics to forecast The workload's cpu load should be predictable, too low or too unstable workload often is unpredictable","title":"Analytics and Recommend HPA"},{"location":"zh/tutorials/dynamic-scheduler-plugin/","text":"Dynamic-scheduler: a load-aware scheduler plugin \u00b6 Introduction \u00b6 Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems: for some nodes, the actual load is not much different from the resource request, which will lead to a very high probability of stability problems. for others, the actual load is much smaller than the resource request, which will lead to a huge waste of resources. To solve these problems, Dynamic scheduler builds a simple but efficient model based on actual node utilization data\uff0cand filters out those nodes with high load to balance the cluster. Design Details \u00b6 Architecture \u00b6 As shown above, Dynamic scheduler relies on Prometheus and Node-exporter to collect and aggregate metrics data, and it consists of two components: Note Node-annotator is currently a module of Crane-scheduler-controller . Node-annotator periodically pulls data from Prometheus and marks them with timestamp on the node in the form of annotations. Dynamic plugin reads the load data directly from the node's annotation, filters and scores candidates based on a simple algorithm. Scheduler Policy \u00b6 Dynamic provides a default scheduler policy and supports user-defined policies. The default policy reies on following metrics: cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d At the scheduling Filter stage, the node will be filtered if the actual usage rate of this node is greater than the threshold of any of the above metrics. And at the Score stage, the final score is the weighted sum of these metrics' values. Hot Value \u00b6 In the production cluster, scheduling hotspots may occur frequently because the load of the nodes can not increase immediately after the pod is created. Therefore, we define an extra metrics named Hot Value , which represents the scheduling frequency of the node in recent times. And the final priority of the node is the final score minus the Hot Value .","title":"Dynamic-scheduler: a load-aware scheduler plugin"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#dynamic-scheduler-a-load-aware-scheduler-plugin","text":"","title":"Dynamic-scheduler: a load-aware scheduler plugin"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#introduction","text":"Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems: for some nodes, the actual load is not much different from the resource request, which will lead to a very high probability of stability problems. for others, the actual load is much smaller than the resource request, which will lead to a huge waste of resources. To solve these problems, Dynamic scheduler builds a simple but efficient model based on actual node utilization data\uff0cand filters out those nodes with high load to balance the cluster.","title":"Introduction"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#design-details","text":"","title":"Design Details"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#architecture","text":"As shown above, Dynamic scheduler relies on Prometheus and Node-exporter to collect and aggregate metrics data, and it consists of two components: Note Node-annotator is currently a module of Crane-scheduler-controller . Node-annotator periodically pulls data from Prometheus and marks them with timestamp on the node in the form of annotations. Dynamic plugin reads the load data directly from the node's annotation, filters and scores candidates based on a simple algorithm.","title":"Architecture"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#scheduler-policy","text":"Dynamic provides a default scheduler policy and supports user-defined policies. The default policy reies on following metrics: cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d At the scheduling Filter stage, the node will be filtered if the actual usage rate of this node is greater than the threshold of any of the above metrics. And at the Score stage, the final score is the weighted sum of these metrics' values.","title":"Scheduler Policy"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#hot-value","text":"In the production cluster, scheduling hotspots may occur frequently because the load of the nodes can not increase immediately after the pod is created. Therefore, we define an extra metrics named Hot Value , which represents the scheduling frequency of the node in recent times. And the final priority of the node is the final score minus the Hot Value .","title":"Hot Value"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/","text":"Crane-scheduler \u00b6 Overview \u00b6 Crane-scheduler is a collection of scheduler plugins based on scheduler framework , including: Dynamic scheuler: a load-aware scheduler plugin Get Started \u00b6 1. Install Prometheus \u00b6 Make sure your kubernetes cluster has Prometheus installed. If not, please refer to Install Prometheus . 2. Configure Prometheus Rules \u00b6 1) Configure the rules of Prometheus to get expected aggregated data: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \u26a0\ufe0fTroubleshooting: The sampling interval of Prometheus must be less than 30 seconds, otherwise the above rules(such as cpu_usage_active) may not take effect. 2) Update the configuration of Prometheus service discovery to ensure that node_exporters/telegraf are using node name as instance name: - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note: This step can be skipped if the node name itself is the host IP. 3. Install Crane-scheduler \u00b6 There are two options: 1) Install Crane-scheduler as a second scheduler: helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler 2) Replace native Kube-scheduler with Crane-scheduler: 1) Backup /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ 2) Modify configfile of kube-scheduler( scheduler-config.yaml ) to enable Dynamic scheduler plugin and configure plugin args: apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... 3) Create /etc/kubernetes/policy.yaml , using as scheduler policy of Dynamic plugin: apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 4) Modify kube-scheduler.yaml and replace kube-scheduler image with Crane-scheduler\uff1a ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... 1) Install crane-scheduler-controller : kubectl apply ./deploy/controller/rbac.yaml && kubectl apply -f ./deploy/controller/deployment.yaml 4. Schedule Pods With Crane-scheduler \u00b6 Test Crane-scheduler with following example: apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note: Change crane-scheduler to default-scheduler if crane-scheduler is used as default. There will be the following event if the test pod is successfully scheduled: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"Load-aware Scheduling"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler","text":"","title":"Crane-scheduler"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#overview","text":"Crane-scheduler is a collection of scheduler plugins based on scheduler framework , including: Dynamic scheuler: a load-aware scheduler plugin","title":"Overview"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#get-started","text":"","title":"Get Started"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#1-install-prometheus","text":"Make sure your kubernetes cluster has Prometheus installed. If not, please refer to Install Prometheus .","title":"1. Install Prometheus"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#2-configure-prometheus-rules","text":"1) Configure the rules of Prometheus to get expected aggregated data: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \u26a0\ufe0fTroubleshooting: The sampling interval of Prometheus must be less than 30 seconds, otherwise the above rules(such as cpu_usage_active) may not take effect. 2) Update the configuration of Prometheus service discovery to ensure that node_exporters/telegraf are using node name as instance name: - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note: This step can be skipped if the node name itself is the host IP.","title":"2. Configure Prometheus Rules"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#3-install-crane-scheduler","text":"There are two options: 1) Install Crane-scheduler as a second scheduler: helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler 2) Replace native Kube-scheduler with Crane-scheduler: 1) Backup /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ 2) Modify configfile of kube-scheduler( scheduler-config.yaml ) to enable Dynamic scheduler plugin and configure plugin args: apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... 3) Create /etc/kubernetes/policy.yaml , using as scheduler policy of Dynamic plugin: apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 4) Modify kube-scheduler.yaml and replace kube-scheduler image with Crane-scheduler\uff1a ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... 1) Install crane-scheduler-controller : kubectl apply ./deploy/controller/rbac.yaml && kubectl apply -f ./deploy/controller/deployment.yaml","title":"3. Install Crane-scheduler"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#4-schedule-pods-with-crane-scheduler","text":"Test Crane-scheduler with following example: apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note: Change crane-scheduler to default-scheduler if crane-scheduler is used as default. There will be the following event if the test pod is successfully scheduled: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"4. Schedule Pods With Crane-scheduler"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/","text":"EffectiveHorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler\uff08\u7b80\u79f0 EHPA\uff09\u662f Crane \u63d0\u4f9b\u7684\u5f39\u6027\u4f38\u7f29\u4ea7\u54c1\uff0c\u5b83\u57fa\u4e8e\u793e\u533a HPA \u505a\u5e95\u5c42\u7684\u5f39\u6027\u63a7\u5236\uff0c\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u5f39\u6027\u89e6\u53d1\u7b56\u7565\uff08\u9884\u6d4b\uff0c\u89c2\u6d4b\uff0c\u5468\u671f\uff09\uff0c\u8ba9\u5f39\u6027\u66f4\u52a0\u9ad8\u6548\uff0c\u5e76\u4fdd\u969c\u4e86\u670d\u52a1\u7684\u8d28\u91cf\u3002 \u63d0\u524d\u6269\u5bb9\uff0c\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\uff1a\u901a\u8fc7\u7b97\u6cd5\u9884\u6d4b\u672a\u6765\u7684\u6d41\u91cf\u6d2a\u5cf0\u63d0\u524d\u6269\u5bb9\uff0c\u907f\u514d\u6269\u5bb9\u4e0d\u53ca\u65f6\u5bfc\u81f4\u7684\u96ea\u5d29\u548c\u670d\u52a1\u7a33\u5b9a\u6027\u6545\u969c\u3002 \u51cf\u5c11\u65e0\u6548\u7f29\u5bb9\uff1a\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u53ef\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7f29\u5bb9\uff0c\u7a33\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8d44\u6e90\u4f7f\u7528\u7387\uff0c\u6d88\u9664\u7a81\u523a\u8bef\u5224\u3002 \u652f\u6301 Cron \u914d\u7f6e\uff1a\u652f\u6301 Cron-based \u5f39\u6027\u914d\u7f6e\uff0c\u5e94\u5bf9\u5927\u4fc3\u7b49\u5f02\u5e38\u6d41\u91cf\u6d2a\u5cf0\u3002 \u517c\u5bb9\u793e\u533a\uff1a\u4f7f\u7528\u793e\u533a HPA \u4f5c\u4e3a\u5f39\u6027\u63a7\u5236\u7684\u6267\u884c\u5c42\uff0c\u80fd\u529b\u5b8c\u5168\u517c\u5bb9\u793e\u533a\u3002 \u4ea7\u54c1\u529f\u80fd \u00b6 \u4e00\u4e2a\u7b80\u5355\u7684 EHPA yaml \u6587\u4ef6\u5982\u4e0b\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef \u914d\u7f6e\u4f60\u5e0c\u671b\u5f39\u6027\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002 MinReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u7f29\u5bb9\u7684\u6700\u5c0f\u503c\u3002 MaxReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u6269\u5bb9\u7684\u6700\u5927\u503c\u3002 ScaleStrategy \u5b9a\u4e49\u4e86\u5f39\u6027\u7684\u7b56\u7565\uff0c\u503c\u53ef\u4ee5\u662f \"Auto\" and \"Preview\". Metrics \u5b9a\u4e49\u4e86\u5f39\u6027\u9608\u503c\u914d\u7f6e\u3002 Prediction \u5b9a\u4e49\u4e86\u9884\u6d4b\u7b97\u6cd5\u914d\u7f6e\u3002 PredictionWindowSeconds \u6307\u5b9a\u5f80\u540e\u9884\u6d4b\u591a\u4e45\u7684\u6570\u636e\u3002 \u57fa\u4e8e\u9884\u6d4b\u7684\u5f39\u6027 \u00b6 \u5927\u591a\u6570\u5728\u7ebf\u5e94\u7528\u7684\u8d1f\u8f7d\u90fd\u6709\u5468\u671f\u6027\u7684\u7279\u5f81\u3002\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6309\u5929\u6216\u8005\u6309\u5468\u7684\u8d8b\u52bf\u9884\u6d4b\u672a\u6765\u7684\u8d1f\u8f7d\u3002EHPA \u4f7f\u7528 DSP \u7b97\u6cd5\u6765\u9884\u6d4b\u5e94\u7528\u672a\u6765\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u5f00\u542f\u4e86\u9884\u6d4b\u80fd\u529b\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" \u76d1\u63a7\u6570\u636e\u515c\u5e95 \u00b6 \u5728\u4f7f\u7528\u9884\u6d4b\u7b97\u6cd5\u9884\u6d4b\u65f6\uff0c\u4f60\u53ef\u80fd\u4f1a\u62c5\u5fc3\u9884\u6d4b\u6570\u636e\u4e0d\u51c6\u5e26\u6765\u4e00\u5b9a\u7684\u98ce\u9669\uff0cEHPA \u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\uff0c\u4e0d\u4ec5\u4f1a\u6309\u9884\u6d4b\u6570\u636e\u8ba1\u7b97\uff0c\u540c\u65f6\u4e5f\u4f1a\u8003\u8651\u5b9e\u9645\u76d1\u63a7\u6570\u636e\u6765\u515c\u5e95\uff0c\u63d0\u5347\u4e86\u5f39\u6027\u7684\u5b89\u5168\u6027\u3002 \u5b9e\u73b0\u7684\u539f\u7406\u662f\u5f53\u4f60\u5728 EHPA \u4e2d\u5b9a\u4e49 spec.metrics \u5e76\u4e14\u5f00\u542f\u5f39\u6027\u9884\u6d4b\u65f6\uff0cEffectiveHPAController \u4f1a\u5728\u521b\u5efa\u5e95\u5c42\u7ba1\u7406\u7684 HPA \u65f6\u6309\u7b56\u7565\u81ea\u52a8\u751f\u6210\u591a\u6761 Metric Spec\u3002 \u4f8b\u5982\uff0c\u5f53\u7528\u6237\u5728 EHPA \u7684 yaml \u91cc\u5b9a\u4e49\u5982\u4e0b Metric Spec\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 \u5b83\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u4e24\u6761 HPA \u7684\u9608\u503c\u914d\u7f6e\uff1a apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource \u5728\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u7528\u6237\u5728 EHPA \u521b\u5efa\u7684 Metric \u9608\u503c\u914d\u7f6e\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u5e95\u5c42 HPA \u4e0a\u7684\u4e24\u6761 Metric \u9608\u503c\u914d\u7f6e\uff1a\u9884\u6d4b Metric \u9608\u503c\u548c\u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u9884\u6d4b Metric \u9608\u503c \u662f\u4e00\u4e2a custom metric\u3002\u503c\u901a\u8fc7 Crane \u7684 MetricAdapter \u63d0\u4f9b\u3002 \u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u662f\u4e00\u4e2a resource metric\uff0c\u5b83\u548c\u7528\u6237\u5728 EHPA \u4e0a\u5b9a\u4e49\u7684\u4e00\u6837\u3002\u8fd9\u6837 HPA \u4f1a\u6839\u636e\u5e94\u7528\u5b9e\u9645\u76d1\u63a7\u7684 Metric \u8ba1\u7b97\u526f\u672c\u6570\u3002 HPA \u5728\u914d\u7f6e\u4e86\u591a\u4e2a\u5f39\u6027 Metric \u9608\u503c\u65f6\uff0c\u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\u4f1a\u5206\u522b\u8ba1\u7b97\u6bcf\u6761 Metric \u5bf9\u5e94\u7684\u526f\u672c\u6570\uff0c\u5e76\u9009\u62e9 \u6700\u5927 \u7684\u90a3\u4e2a\u526f\u672c\u6570\u4f5c\u4e3a\u6700\u7ec8\u7684\u63a8\u8350\u5f39\u6027\u7ed3\u679c\u3002 \u6c34\u5e73\u5f39\u6027\u7684\u6267\u884c\u6d41\u7a0b \u00b6 EffectiveHPAController \u521b\u5efa HorizontalPodAutoscaler \u548c TimeSeriesPrediction \u5bf9\u8c61 PredictionCore \u4ece prometheus \u83b7\u53d6\u5386\u53f2 metric \u901a\u8fc7\u9884\u6d4b\u7b97\u6cd5\u8ba1\u7b97\uff0c\u5c06\u7ed3\u679c\u8bb0\u5f55\u5230 TimeSeriesPrediction HPAController \u901a\u8fc7 metric client \u4ece KubeApiServer \u8bfb\u53d6 metric \u6570\u636e KubeApiServer \u5c06\u8bf7\u6c42\u8def\u7531\u5230 Crane \u7684 MetricAdapter\u3002 HPAController \u8ba1\u7b97\u6240\u6709\u7684 Metric \u8fd4\u56de\u7684\u7ed3\u679c\u5f97\u5230\u6700\u7ec8\u7684\u5f39\u6027\u526f\u672c\u63a8\u8350\u3002 HPAController \u8c03\u7528 scale API \u5bf9\u76ee\u6807\u5e94\u7528\u6269/\u7f29\u5bb9\u3002 \u6574\u4f53\u6d41\u7a0b\u56fe\u5982\u4e0b\uff1a \u7528\u6237\u6848\u4f8b \u00b6 \u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u751f\u4ea7\u73af\u5883\u7684\u5ba2\u6237\u6848\u4f8b\u6765\u4ecb\u7ecd EHPA \u7684\u843d\u5730\u6548\u679c\u3002 \u6211\u4eec\u5c06\u751f\u4ea7\u4e0a\u7684\u6570\u636e\u5728\u9884\u53d1\u73af\u5883\u91cd\u653e\uff0c\u5bf9\u6bd4\u4f7f\u7528 EHPA \u548c\u793e\u533a\u7684 HPA \u7684\u5f39\u6027\u6548\u679c\u3002 \u4e0b\u56fe\u7684\u7ea2\u7ebf\u662f\u5e94\u7528\u5728\u4e00\u5929\u5185\u7684\u5b9e\u9645 CPU \u4f7f\u7528\u91cf\u66f2\u7ebf\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u57288\u70b9\uff0c12\u70b9\uff0c\u665a\u4e0a8\u70b9\u65f6\u662f\u4f7f\u7528\u9ad8\u5cf0\u3002\u7eff\u7ebf\u662f EHPA \u9884\u6d4b\u7684 CPU \u4f7f\u7528\u91cf\u3002 \u4e0b\u56fe\u662f\u5bf9\u5e94\u7684\u81ea\u52a8\u5f39\u6027\u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7ea2\u7ebf\u662f\u793e\u533a HPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7eff\u7ebf\u662f EHPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\u3002 \u53ef\u4ee5\u770b\u5230 EHPA \u5177\u6709\u4ee5\u4e0b\u4f18\u52bf\uff1a \u5728\u6d41\u91cf\u6d2a\u5cf0\u6765\u4e34\u524d\u6269\u5bb9\u3002 \u5f53\u6d41\u91cf\u5148\u964d\u540e\u7acb\u523b\u5347\u65f6\u4e0d\u505a\u65e0\u6548\u7f29\u5bb9\u3002 \u76f8\u6bd4 HPA \u66f4\u5c11\u7684\u5f39\u6027\u6b21\u6570\u5374\u66f4\u9ad8\u6548\u3002 ScaleStrategy \u5f39\u6027\u7b56\u7565 \u00b6 EHPA \u63d0\u4f9b\u4e86\u4e24\u79cd\u5f39\u6027\u7b56\u7565\uff1a Auto \u548c Preview \u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5207\u6362\u5b83\u5e76\u7acb\u5373\u751f\u6548\u3002 Auto \u00b6 Auto \u7b56\u7565\u4e0b EHPA \u4f1a\u81ea\u52a8\u6267\u884c\u5f39\u6027\u884c\u4e3a\u3002\u9ed8\u8ba4 EHPA \u7684\u7b56\u7565\u662f Auto\u3002\u5728\u8fd9\u4e2a\u6a21\u5f0f\u4e0b EHPA \u4f1a\u521b\u5efa\u4e00\u4e2a\u793e\u533a\u7684 HPA \u5bf9\u8c61\u5e76\u81ea\u52a8\u63a5\u7ba1\u5b83\u7684\u751f\u547d\u5468\u671f\u3002\u6211\u4eec\u4e0d\u5efa\u8bae\u7528\u6237\u4fee\u6539\u6216\u8005\u63a7\u5236\u8fd9\u4e2a\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\uff0c\u5f53 EHPA \u88ab\u5220\u9664\u65f6\uff0c\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\u4e5f\u4f1a\u4e00\u5e76\u5220\u9664\u3002 Preview \u00b6 Preview \u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba9 EHPA \u4e0d\u81ea\u52a8\u6267\u884c\u5f39\u6027\u7684\u80fd\u529b\u3002\u6240\u4ee5\u4f60\u53ef\u4ee5\u901a\u8fc7 EHPA \u7684 desiredReplicas \u5b57\u6bb5\u89c2\u6d4b EHPA \u8ba1\u7b97\u51fa\u7684\u526f\u672c\u6570\u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5728\u4e24\u4e2a\u6a21\u5f0f\u95f4\u5207\u6362\uff0c\u5f53\u7528\u6237\u5207\u6362\u5230 Preview \u6a21\u5f0f\u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7 spec.specificReplicas \u8c03\u6574\u5e94\u7528\u7684\u526f\u672c\u6570\uff0c\u5982\u679c spec.specificReplicas \u4e3a\u7a7a\uff0c\u5219\u4e0d\u4f1a\u5bf9\u5e94\u7528\u6267\u884c\u5f39\u6027\uff0c\u4f46\u662f\u4f9d\u7136\u4f1a\u6267\u884c\u526f\u672c\u6570\u7684\u8ba1\u7b97\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u914d\u7f6e\u6210 Preview \u6a21\u5f0f\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target HorizontalPodAutoscaler \u793e\u533a\u517c\u5bb9 \u00b6 EHPA \u4ece\u8bbe\u8ba1\u4e4b\u51fa\u5c31\u5e0c\u671b\u548c\u793e\u533a\u7684 HPA \u517c\u5bb9\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u5e0c\u671b\u91cd\u65b0\u9020\u4e00\u4e2a\u7c7b\u4f3c HPA \u7684\u8f6e\u5b50\uff0cHPA \u5728\u4e0d\u65ad\u6f14\u8fdb\u7684\u8fc7\u7a0b\u5df2\u7ecf\u89e3\u51b3\u4e86\u5f88\u591a\u901a\u7528\u7684\u95ee\u9898\uff0cEHPA \u5e0c\u671b\u5728 HPA \u7684\u57fa\u7840\u4e0a\u63d0\u4f9b\u66f4\u9ad8\u9636\u7684 CRD\uff0cEHPA \u7684\u529f\u80fd\u662f\u793e\u533a HPA \u7684\u8d85\u96c6\u3002 EHPA \u4e5f\u4f1a\u6301\u7eed\u8ddf\u8fdb\u652f\u6301 HPA \u7684\u65b0\u529f\u80fd\u3002 EffectiveHorizontalPodAutoscaler status \u00b6 EHPA \u7684 Status \u5305\u62ec\u4e86\u81ea\u8eab\u7684 Status \u540c\u65f6\u4e5f\u6c47\u805a\u4e86\u5e95\u5c42 HPA \u7684\u90e8\u5206 Status\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a EHPA \u7684 Status yaml\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0","title":"Effective HPA"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler\uff08\u7b80\u79f0 EHPA\uff09\u662f Crane \u63d0\u4f9b\u7684\u5f39\u6027\u4f38\u7f29\u4ea7\u54c1\uff0c\u5b83\u57fa\u4e8e\u793e\u533a HPA \u505a\u5e95\u5c42\u7684\u5f39\u6027\u63a7\u5236\uff0c\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u5f39\u6027\u89e6\u53d1\u7b56\u7565\uff08\u9884\u6d4b\uff0c\u89c2\u6d4b\uff0c\u5468\u671f\uff09\uff0c\u8ba9\u5f39\u6027\u66f4\u52a0\u9ad8\u6548\uff0c\u5e76\u4fdd\u969c\u4e86\u670d\u52a1\u7684\u8d28\u91cf\u3002 \u63d0\u524d\u6269\u5bb9\uff0c\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\uff1a\u901a\u8fc7\u7b97\u6cd5\u9884\u6d4b\u672a\u6765\u7684\u6d41\u91cf\u6d2a\u5cf0\u63d0\u524d\u6269\u5bb9\uff0c\u907f\u514d\u6269\u5bb9\u4e0d\u53ca\u65f6\u5bfc\u81f4\u7684\u96ea\u5d29\u548c\u670d\u52a1\u7a33\u5b9a\u6027\u6545\u969c\u3002 \u51cf\u5c11\u65e0\u6548\u7f29\u5bb9\uff1a\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u53ef\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7f29\u5bb9\uff0c\u7a33\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8d44\u6e90\u4f7f\u7528\u7387\uff0c\u6d88\u9664\u7a81\u523a\u8bef\u5224\u3002 \u652f\u6301 Cron \u914d\u7f6e\uff1a\u652f\u6301 Cron-based \u5f39\u6027\u914d\u7f6e\uff0c\u5e94\u5bf9\u5927\u4fc3\u7b49\u5f02\u5e38\u6d41\u91cf\u6d2a\u5cf0\u3002 \u517c\u5bb9\u793e\u533a\uff1a\u4f7f\u7528\u793e\u533a HPA \u4f5c\u4e3a\u5f39\u6027\u63a7\u5236\u7684\u6267\u884c\u5c42\uff0c\u80fd\u529b\u5b8c\u5168\u517c\u5bb9\u793e\u533a\u3002","title":"EffectiveHorizontalPodAutoscaler"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_1","text":"\u4e00\u4e2a\u7b80\u5355\u7684 EHPA yaml \u6587\u4ef6\u5982\u4e0b\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef \u914d\u7f6e\u4f60\u5e0c\u671b\u5f39\u6027\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002 MinReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u7f29\u5bb9\u7684\u6700\u5c0f\u503c\u3002 MaxReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u6269\u5bb9\u7684\u6700\u5927\u503c\u3002 ScaleStrategy \u5b9a\u4e49\u4e86\u5f39\u6027\u7684\u7b56\u7565\uff0c\u503c\u53ef\u4ee5\u662f \"Auto\" and \"Preview\". Metrics \u5b9a\u4e49\u4e86\u5f39\u6027\u9608\u503c\u914d\u7f6e\u3002 Prediction \u5b9a\u4e49\u4e86\u9884\u6d4b\u7b97\u6cd5\u914d\u7f6e\u3002 PredictionWindowSeconds \u6307\u5b9a\u5f80\u540e\u9884\u6d4b\u591a\u4e45\u7684\u6570\u636e\u3002","title":"\u4ea7\u54c1\u529f\u80fd"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_2","text":"\u5927\u591a\u6570\u5728\u7ebf\u5e94\u7528\u7684\u8d1f\u8f7d\u90fd\u6709\u5468\u671f\u6027\u7684\u7279\u5f81\u3002\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6309\u5929\u6216\u8005\u6309\u5468\u7684\u8d8b\u52bf\u9884\u6d4b\u672a\u6765\u7684\u8d1f\u8f7d\u3002EHPA \u4f7f\u7528 DSP \u7b97\u6cd5\u6765\u9884\u6d4b\u5e94\u7528\u672a\u6765\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u5f00\u542f\u4e86\u9884\u6d4b\u80fd\u529b\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\"","title":"\u57fa\u4e8e\u9884\u6d4b\u7684\u5f39\u6027"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_3","text":"\u5728\u4f7f\u7528\u9884\u6d4b\u7b97\u6cd5\u9884\u6d4b\u65f6\uff0c\u4f60\u53ef\u80fd\u4f1a\u62c5\u5fc3\u9884\u6d4b\u6570\u636e\u4e0d\u51c6\u5e26\u6765\u4e00\u5b9a\u7684\u98ce\u9669\uff0cEHPA \u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\uff0c\u4e0d\u4ec5\u4f1a\u6309\u9884\u6d4b\u6570\u636e\u8ba1\u7b97\uff0c\u540c\u65f6\u4e5f\u4f1a\u8003\u8651\u5b9e\u9645\u76d1\u63a7\u6570\u636e\u6765\u515c\u5e95\uff0c\u63d0\u5347\u4e86\u5f39\u6027\u7684\u5b89\u5168\u6027\u3002 \u5b9e\u73b0\u7684\u539f\u7406\u662f\u5f53\u4f60\u5728 EHPA \u4e2d\u5b9a\u4e49 spec.metrics \u5e76\u4e14\u5f00\u542f\u5f39\u6027\u9884\u6d4b\u65f6\uff0cEffectiveHPAController \u4f1a\u5728\u521b\u5efa\u5e95\u5c42\u7ba1\u7406\u7684 HPA \u65f6\u6309\u7b56\u7565\u81ea\u52a8\u751f\u6210\u591a\u6761 Metric Spec\u3002 \u4f8b\u5982\uff0c\u5f53\u7528\u6237\u5728 EHPA \u7684 yaml \u91cc\u5b9a\u4e49\u5982\u4e0b Metric Spec\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 \u5b83\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u4e24\u6761 HPA \u7684\u9608\u503c\u914d\u7f6e\uff1a apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource \u5728\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u7528\u6237\u5728 EHPA \u521b\u5efa\u7684 Metric \u9608\u503c\u914d\u7f6e\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u5e95\u5c42 HPA \u4e0a\u7684\u4e24\u6761 Metric \u9608\u503c\u914d\u7f6e\uff1a\u9884\u6d4b Metric \u9608\u503c\u548c\u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u9884\u6d4b Metric \u9608\u503c \u662f\u4e00\u4e2a custom metric\u3002\u503c\u901a\u8fc7 Crane \u7684 MetricAdapter \u63d0\u4f9b\u3002 \u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u662f\u4e00\u4e2a resource metric\uff0c\u5b83\u548c\u7528\u6237\u5728 EHPA \u4e0a\u5b9a\u4e49\u7684\u4e00\u6837\u3002\u8fd9\u6837 HPA \u4f1a\u6839\u636e\u5e94\u7528\u5b9e\u9645\u76d1\u63a7\u7684 Metric \u8ba1\u7b97\u526f\u672c\u6570\u3002 HPA \u5728\u914d\u7f6e\u4e86\u591a\u4e2a\u5f39\u6027 Metric \u9608\u503c\u65f6\uff0c\u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\u4f1a\u5206\u522b\u8ba1\u7b97\u6bcf\u6761 Metric \u5bf9\u5e94\u7684\u526f\u672c\u6570\uff0c\u5e76\u9009\u62e9 \u6700\u5927 \u7684\u90a3\u4e2a\u526f\u672c\u6570\u4f5c\u4e3a\u6700\u7ec8\u7684\u63a8\u8350\u5f39\u6027\u7ed3\u679c\u3002","title":"\u76d1\u63a7\u6570\u636e\u515c\u5e95"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_4","text":"EffectiveHPAController \u521b\u5efa HorizontalPodAutoscaler \u548c TimeSeriesPrediction \u5bf9\u8c61 PredictionCore \u4ece prometheus \u83b7\u53d6\u5386\u53f2 metric \u901a\u8fc7\u9884\u6d4b\u7b97\u6cd5\u8ba1\u7b97\uff0c\u5c06\u7ed3\u679c\u8bb0\u5f55\u5230 TimeSeriesPrediction HPAController \u901a\u8fc7 metric client \u4ece KubeApiServer \u8bfb\u53d6 metric \u6570\u636e KubeApiServer \u5c06\u8bf7\u6c42\u8def\u7531\u5230 Crane \u7684 MetricAdapter\u3002 HPAController \u8ba1\u7b97\u6240\u6709\u7684 Metric \u8fd4\u56de\u7684\u7ed3\u679c\u5f97\u5230\u6700\u7ec8\u7684\u5f39\u6027\u526f\u672c\u63a8\u8350\u3002 HPAController \u8c03\u7528 scale API \u5bf9\u76ee\u6807\u5e94\u7528\u6269/\u7f29\u5bb9\u3002 \u6574\u4f53\u6d41\u7a0b\u56fe\u5982\u4e0b\uff1a","title":"\u6c34\u5e73\u5f39\u6027\u7684\u6267\u884c\u6d41\u7a0b"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_5","text":"\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u751f\u4ea7\u73af\u5883\u7684\u5ba2\u6237\u6848\u4f8b\u6765\u4ecb\u7ecd EHPA \u7684\u843d\u5730\u6548\u679c\u3002 \u6211\u4eec\u5c06\u751f\u4ea7\u4e0a\u7684\u6570\u636e\u5728\u9884\u53d1\u73af\u5883\u91cd\u653e\uff0c\u5bf9\u6bd4\u4f7f\u7528 EHPA \u548c\u793e\u533a\u7684 HPA \u7684\u5f39\u6027\u6548\u679c\u3002 \u4e0b\u56fe\u7684\u7ea2\u7ebf\u662f\u5e94\u7528\u5728\u4e00\u5929\u5185\u7684\u5b9e\u9645 CPU \u4f7f\u7528\u91cf\u66f2\u7ebf\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u57288\u70b9\uff0c12\u70b9\uff0c\u665a\u4e0a8\u70b9\u65f6\u662f\u4f7f\u7528\u9ad8\u5cf0\u3002\u7eff\u7ebf\u662f EHPA \u9884\u6d4b\u7684 CPU \u4f7f\u7528\u91cf\u3002 \u4e0b\u56fe\u662f\u5bf9\u5e94\u7684\u81ea\u52a8\u5f39\u6027\u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7ea2\u7ebf\u662f\u793e\u533a HPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7eff\u7ebf\u662f EHPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\u3002 \u53ef\u4ee5\u770b\u5230 EHPA \u5177\u6709\u4ee5\u4e0b\u4f18\u52bf\uff1a \u5728\u6d41\u91cf\u6d2a\u5cf0\u6765\u4e34\u524d\u6269\u5bb9\u3002 \u5f53\u6d41\u91cf\u5148\u964d\u540e\u7acb\u523b\u5347\u65f6\u4e0d\u505a\u65e0\u6548\u7f29\u5bb9\u3002 \u76f8\u6bd4 HPA \u66f4\u5c11\u7684\u5f39\u6027\u6b21\u6570\u5374\u66f4\u9ad8\u6548\u3002","title":"\u7528\u6237\u6848\u4f8b"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#scalestrategy","text":"EHPA \u63d0\u4f9b\u4e86\u4e24\u79cd\u5f39\u6027\u7b56\u7565\uff1a Auto \u548c Preview \u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5207\u6362\u5b83\u5e76\u7acb\u5373\u751f\u6548\u3002","title":"ScaleStrategy \u5f39\u6027\u7b56\u7565"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#auto","text":"Auto \u7b56\u7565\u4e0b EHPA \u4f1a\u81ea\u52a8\u6267\u884c\u5f39\u6027\u884c\u4e3a\u3002\u9ed8\u8ba4 EHPA \u7684\u7b56\u7565\u662f Auto\u3002\u5728\u8fd9\u4e2a\u6a21\u5f0f\u4e0b EHPA \u4f1a\u521b\u5efa\u4e00\u4e2a\u793e\u533a\u7684 HPA \u5bf9\u8c61\u5e76\u81ea\u52a8\u63a5\u7ba1\u5b83\u7684\u751f\u547d\u5468\u671f\u3002\u6211\u4eec\u4e0d\u5efa\u8bae\u7528\u6237\u4fee\u6539\u6216\u8005\u63a7\u5236\u8fd9\u4e2a\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\uff0c\u5f53 EHPA \u88ab\u5220\u9664\u65f6\uff0c\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\u4e5f\u4f1a\u4e00\u5e76\u5220\u9664\u3002","title":"Auto"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#preview","text":"Preview \u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba9 EHPA \u4e0d\u81ea\u52a8\u6267\u884c\u5f39\u6027\u7684\u80fd\u529b\u3002\u6240\u4ee5\u4f60\u53ef\u4ee5\u901a\u8fc7 EHPA \u7684 desiredReplicas \u5b57\u6bb5\u89c2\u6d4b EHPA \u8ba1\u7b97\u51fa\u7684\u526f\u672c\u6570\u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5728\u4e24\u4e2a\u6a21\u5f0f\u95f4\u5207\u6362\uff0c\u5f53\u7528\u6237\u5207\u6362\u5230 Preview \u6a21\u5f0f\u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7 spec.specificReplicas \u8c03\u6574\u5e94\u7528\u7684\u526f\u672c\u6570\uff0c\u5982\u679c spec.specificReplicas \u4e3a\u7a7a\uff0c\u5219\u4e0d\u4f1a\u5bf9\u5e94\u7528\u6267\u884c\u5f39\u6027\uff0c\u4f46\u662f\u4f9d\u7136\u4f1a\u6267\u884c\u526f\u672c\u6570\u7684\u8ba1\u7b97\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u914d\u7f6e\u6210 Preview \u6a21\u5f0f\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target","title":"Preview"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontalpodautoscaler","text":"EHPA \u4ece\u8bbe\u8ba1\u4e4b\u51fa\u5c31\u5e0c\u671b\u548c\u793e\u533a\u7684 HPA \u517c\u5bb9\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u5e0c\u671b\u91cd\u65b0\u9020\u4e00\u4e2a\u7c7b\u4f3c HPA \u7684\u8f6e\u5b50\uff0cHPA \u5728\u4e0d\u65ad\u6f14\u8fdb\u7684\u8fc7\u7a0b\u5df2\u7ecf\u89e3\u51b3\u4e86\u5f88\u591a\u901a\u7528\u7684\u95ee\u9898\uff0cEHPA \u5e0c\u671b\u5728 HPA \u7684\u57fa\u7840\u4e0a\u63d0\u4f9b\u66f4\u9ad8\u9636\u7684 CRD\uff0cEHPA \u7684\u529f\u80fd\u662f\u793e\u533a HPA \u7684\u8d85\u96c6\u3002 EHPA \u4e5f\u4f1a\u6301\u7eed\u8ddf\u8fdb\u652f\u6301 HPA \u7684\u65b0\u529f\u80fd\u3002","title":"HorizontalPodAutoscaler \u793e\u533a\u517c\u5bb9"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler-status","text":"EHPA \u7684 Status \u5305\u62ec\u4e86\u81ea\u8eab\u7684 Status \u540c\u65f6\u4e5f\u6c47\u805a\u4e86\u5e95\u5c42 HPA \u7684\u90e8\u5206 Status\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a EHPA \u7684 Status yaml\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0","title":"EffectiveHorizontalPodAutoscaler status"},{"location":"zh/tutorials/using-qos-ensurance/","text":"Qos Ensurance \u00b6 QoS ensurance guarantees the stability of the pods running on Kubernetes. Disable schedule, throttle, evict will be applied to low priority pods when the higher priority pods is impacted by resource competition. Qos Ensurance Architecture \u00b6 Qos ensurance's architecture is shown as below. It contains three modules. state collector: collect metrics periodically anomaly analyzer: analyze the node triggered anomaly used collected metrics action executor: execute avoidance actions, include disable scheduling, throttle and eviction. The main process: State collector synchronizes policies from kube-apiserver. If the policies are changed, the state collector updates the collectors. State collector collects metrics periodically. State collector transmits metrics to anomaly analyzer. Anomaly analyzer ranges all rules to analyze the avoidance threshold or the restored threshold reached. Anomaly analyzer merges the analyzed results and notices the avoidance actions. Action executor executes actions based on the analyzed results. Disable Scheduling \u00b6 The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, disable schedule action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 # The minimum wait time of the node from scheduling disable status to normal status apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) We consider the rule is triggered, when the threshold reached continued so many times We consider the rule is restored, when the threshold not reached continued so many times Name of AvoidanceAction which be associated Strategy for the action, you can set it \"Preview\" to not perform actually Name of metric Threshold of metric Please check the video to learn more about the scheduling disable actions. Throttle \u00b6 The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, throttle action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" The minimal ratio of the CPU quota, if the pod is throttled lower than this ratio, it will be set to this. The step for throttle action. It will reduce this percentage of CPU quota in each avoidance triggered.It will increase this percentage of CPU quota in each restored. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000 Eviction \u00b6 The following YAML is another case, low priority pods on the node will be evicted, when the node CPU usage trigger the threshold. apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" Duration in seconds the pod needs to terminate gracefully. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 Strategy for the action, \"Preview\" to not perform actually Supported Metrics \u00b6 Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Qos Ensurance"},{"location":"zh/tutorials/using-qos-ensurance/#qos-ensurance","text":"QoS ensurance guarantees the stability of the pods running on Kubernetes. Disable schedule, throttle, evict will be applied to low priority pods when the higher priority pods is impacted by resource competition.","title":"Qos Ensurance"},{"location":"zh/tutorials/using-qos-ensurance/#qos-ensurance-architecture","text":"Qos ensurance's architecture is shown as below. It contains three modules. state collector: collect metrics periodically anomaly analyzer: analyze the node triggered anomaly used collected metrics action executor: execute avoidance actions, include disable scheduling, throttle and eviction. The main process: State collector synchronizes policies from kube-apiserver. If the policies are changed, the state collector updates the collectors. State collector collects metrics periodically. State collector transmits metrics to anomaly analyzer. Anomaly analyzer ranges all rules to analyze the avoidance threshold or the restored threshold reached. Anomaly analyzer merges the analyzed results and notices the avoidance actions. Action executor executes actions based on the analyzed results.","title":"Qos Ensurance Architecture"},{"location":"zh/tutorials/using-qos-ensurance/#disable-scheduling","text":"The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, disable schedule action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 # The minimum wait time of the node from scheduling disable status to normal status apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) We consider the rule is triggered, when the threshold reached continued so many times We consider the rule is restored, when the threshold not reached continued so many times Name of AvoidanceAction which be associated Strategy for the action, you can set it \"Preview\" to not perform actually Name of metric Threshold of metric Please check the video to learn more about the scheduling disable actions.","title":"Disable Scheduling"},{"location":"zh/tutorials/using-qos-ensurance/#throttle","text":"The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, throttle action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" The minimal ratio of the CPU quota, if the pod is throttled lower than this ratio, it will be set to this. The step for throttle action. It will reduce this percentage of CPU quota in each avoidance triggered.It will increase this percentage of CPU quota in each restored. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000","title":"Throttle"},{"location":"zh/tutorials/using-qos-ensurance/#eviction","text":"The following YAML is another case, low priority pods on the node will be evicted, when the node CPU usage trigger the threshold. apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" Duration in seconds the pod needs to terminate gracefully. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 Strategy for the action, \"Preview\" to not perform actually","title":"Eviction"},{"location":"zh/tutorials/using-qos-ensurance/#supported-metrics","text":"Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Supported Metrics"},{"location":"zh/tutorials/using-time-series-prediction/","text":"TimeSeriesPrediction \u00b6 Knowing the future makes things easier for us. Many businesses are naturally cyclical in time series, especially for those that directly or indirectly serve \"people\". This periodicity is determined by the regularity of people\u2019s daily activities. For example, people are accustomed to ordering take-out at noon and in the evenings; there are always traffic peaks in the morning and evening; even for services that don't have such obvious patterns, such as searching, the amount of requests at night is much lower than that during business hours. For applications related to this kind of business, it is a natural idea to infer the next day's metrics from the historical data of the past few days, or to infer the coming Monday's access traffic from the data of last Monday. With predicted metrics or traffic patterns in the next 24 hours, we can better manage our application instances, stabilize our system, and meanwhile, reduce the cost. TimeSeriesPrediction is used to forecast the kubernetes object metric. It is based on PredictionCore to do forecast. Features \u00b6 A TimeSeriesPrediction sample yaml looks like below: apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef defines the reference to the kubernetes object including Node or other workload such as Deployment. spec.predictionMetrics defines the metrics about the spec.targetRef. spec.predictionWindowSeconds is a prediction time series duration. the TimeSeriesPredictionController will rotate the predicted data in spec.Status for consumer to consume the predicted time series data. PredictionMetrics \u00b6 apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" MetricType \u00b6 There are three types of the metric query: ResourceQuery is a kubernetes built-in resource metric such as cpu or memory. crane supports only cpu and memory now. RawQuery is a query by DSL, such as prometheus query language. now support prometheus. ExpressionQuery is a query by Expression selector. Now we only support prometheus as data source. We define the MetricType to orthogonal with the datasource. but now maybe some datasources do not support the metricType. Algorithm \u00b6 Algorithm define the algorithm type and params to do predict for the metric. Now there are two kinds of algorithms: dsp is an algorithm to forcasting a time series, it is based on FFT(Fast Fourier Transform), it is good at predicting some time series with seasonality and periods. percentile is an algorithm to estimate a time series, and find a recommended value to represent the past time series, it is based on exponentially-decaying weights historgram statistics. it is used to estimate a time series, it is not good at to predict a time sequences, although the percentile can output a time series predicted data, but it is all the same value. so if you want to predict a time sequences, dsp is a better choice. dsp params \u00b6 percentile params \u00b6","title":"Time Series Prediction"},{"location":"zh/tutorials/using-time-series-prediction/#timeseriesprediction","text":"Knowing the future makes things easier for us. Many businesses are naturally cyclical in time series, especially for those that directly or indirectly serve \"people\". This periodicity is determined by the regularity of people\u2019s daily activities. For example, people are accustomed to ordering take-out at noon and in the evenings; there are always traffic peaks in the morning and evening; even for services that don't have such obvious patterns, such as searching, the amount of requests at night is much lower than that during business hours. For applications related to this kind of business, it is a natural idea to infer the next day's metrics from the historical data of the past few days, or to infer the coming Monday's access traffic from the data of last Monday. With predicted metrics or traffic patterns in the next 24 hours, we can better manage our application instances, stabilize our system, and meanwhile, reduce the cost. TimeSeriesPrediction is used to forecast the kubernetes object metric. It is based on PredictionCore to do forecast.","title":"TimeSeriesPrediction"},{"location":"zh/tutorials/using-time-series-prediction/#features","text":"A TimeSeriesPrediction sample yaml looks like below: apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef defines the reference to the kubernetes object including Node or other workload such as Deployment. spec.predictionMetrics defines the metrics about the spec.targetRef. spec.predictionWindowSeconds is a prediction time series duration. the TimeSeriesPredictionController will rotate the predicted data in spec.Status for consumer to consume the predicted time series data.","title":"Features"},{"location":"zh/tutorials/using-time-series-prediction/#predictionmetrics","text":"apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\"","title":"PredictionMetrics"},{"location":"zh/tutorials/using-time-series-prediction/#metrictype","text":"There are three types of the metric query: ResourceQuery is a kubernetes built-in resource metric such as cpu or memory. crane supports only cpu and memory now. RawQuery is a query by DSL, such as prometheus query language. now support prometheus. ExpressionQuery is a query by Expression selector. Now we only support prometheus as data source. We define the MetricType to orthogonal with the datasource. but now maybe some datasources do not support the metricType.","title":"MetricType"},{"location":"zh/tutorials/using-time-series-prediction/#algorithm","text":"Algorithm define the algorithm type and params to do predict for the metric. Now there are two kinds of algorithms: dsp is an algorithm to forcasting a time series, it is based on FFT(Fast Fourier Transform), it is good at predicting some time series with seasonality and periods. percentile is an algorithm to estimate a time series, and find a recommended value to represent the past time series, it is based on exponentially-decaying weights historgram statistics. it is used to estimate a time series, it is not good at to predict a time sequences, although the percentile can output a time series predicted data, but it is all the same value. so if you want to predict a time sequences, dsp is a better choice.","title":"Algorithm"},{"location":"zh/tutorials/using-time-series-prediction/#dsp-params","text":"","title":"dsp params"},{"location":"zh/tutorials/using-time-series-prediction/#percentile-params","text":"","title":"percentile params"}]}